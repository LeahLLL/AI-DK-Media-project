# -*- coding: utf-8 -*-
"""Sentiment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18vFknuu1XPdo8pRkuQEWzJhaEPbG-4Vo
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/danishnews.csv')

!pip install spacy
!python -m spacy download da_core_news_lg

import spacy
nlp = spacy.load("da_core_news_lg")

def extract_org_entities(text, max_chars=1500):
    if not isinstance(text, str):
        return []
    doc = nlp(text[:max_chars])
    return list({
        ent.text.strip()
        for ent in doc.ents
        if ent.label_ == "ORG" and len(ent.text.strip()) > 2
    })

df_sample = df.sample(10000, random_state=42)
df_sample["org_entities"] = df_sample["plain_text"].apply(extract_org_entities)

df_sample.to_pickle("/content/drive/MyDrive/df_sample_with_orgs.pkl")

# from collections import Counter

# all_orgs = [o for sub in df_sample["org_entities"] for o in sub]
# org_counts = Counter(all_orgs)

# MEDIA_PATTERNS = [
#     "avis", "blad", "bt", "b.t", "berlingske", "politiken",
#     "jyllands", "reuters", "ritzau", "afp", "bbc", "cnn",
#     "tv2", "dr", ".dk", "radio"
# ]

# SPORTS_PATTERNS = [
#     "fc ", "f.c", "aab", "agf", "dbu", "superliga",
#     "league", "champions", "united", "city", "boldklub"
# ]

# PUBLIC_PATTERNS = [
#     "styrelsen", "institut", "ministerium", "direktorat",
#     "politi", "region", "kommune", "folketing", "universitet"
# ]

# def classify_actor(org):
#     o = org.lower()
#     if any(p in o for p in MEDIA_PATTERNS): return "Media"
#     if any(p in o for p in SPORTS_PATTERNS): return "Sports"
#     if any(p in o for p in PUBLIC_PATTERNS): return "Public Authority"
#     return "Company"

# actor_df = pd.DataFrame([
#     (org, cnt, classify_actor(org))
#     for org, cnt in org_counts.items()
# ], columns=["organization", "count", "actor_type"])

# companies_top = (
#     actor_df
#     .query("actor_type == 'Company'")
#     .sort_values("count", ascending=False)
#     .head(150)
#     .reset_index(drop=True)
# )

MEDIA_PATTERNS = [
    "avis", "blad", "bt", "b.t", "berlingske", "politiken",
    "jyllands", "reuters", "ritzau", "afp", "bbc", "cnn",
    "tv2", "dr", ".dk", "radio"
]

SPORTS_PATTERNS = [
    "fc ", "f.c", "aab", "agf", "dbu", "superliga",
    "league", "champions", "united", "city", "boldklub"
]

PUBLIC_PATTERNS = [
    "styrelsen", "institut", "ministerium", "direktorat",
    "politi", "region", "kommune", "folketing", "universitet"
]

def classify_actor(org):
    o = org.lower().strip()

    if any(p in o for p in MEDIA_PATTERNS):
        return "Media"

    if any(p in o for p in SPORTS_PATTERNS):
        return "Sports"

    if any(p in o for p in PUBLIC_PATTERNS):
        return "Public Authority"

    return "Company"

org_counts = Counter(all_orgs)

actor_rows = []
for org, count in org_counts.items():
    actor = classify_actor(org)
    actor_rows.append((org, count, actor))

actor_df = pd.DataFrame(
    actor_rows,
    columns=["organization", "count", "actor_type"]
)

actor_df.sort_values("count", ascending=False).head(30)

!pip install vaderSentiment -q
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

def get_sentiment(text):
    if not isinstance(text, str) or len(text) < 50:
        return np.nan
    return analyzer.polarity_scores(text)['compound']

df_sample['sentiment'] = df_sample['plain_text'].apply(get_sentiment)
df_sample['sentiment'].describe()

# Explode org_entities to have one row per organization
df_exploded = df_sample.explode('org_entities').dropna(subset=['org_entities'])

# Map actor type
actor_type_dict = dict(zip(actor_df['organization'], actor_df['actor_type']))
df_exploded['actor_type'] = df_exploded['org_entities'].map(actor_type_dict).fillna('Other')

# Keep only companies, sports, and public authorities for analysis
df_exploded = df_exploded[df_exploded['actor_type'].isin(['Company', 'Sports', 'Public Authority'])]

companies = actor_df.query("actor_type == 'Company'")["organization"].tolist()
company_set = set(companies)

df_companies = df_sample.explode("org_entities").dropna(subset=["org_entities"])

df_companies = df_companies[
    df_companies["org_entities"].isin(company_set)
]

print(df_companies.shape)
df_companies.head()

AI_KEYWORDS = ["AI", "kunstig intelligens", "maskinlæring", "dyb læring", "neural netværk", "automatisering",
                  "robotik", "dataanalyse", "algoritme", "intelligente systemer", "GPT", "OPENAI", "LLM", "chatbot",
                    "sprogmodel", "generativ AI", "AI-assistent", "AI-drevet", "computer vision", "naturlig sprogbehandling",
                    "AI-platform", "AI-teknologi", "AI-forskning", "AI-innovation", "AI-applikationer", "AI-løsninger",
                    "AI-udvikling", "AI-sikkerhed", "AI-etik", "AI-regulering", "AI-politik", "AI-strategi", "AI-investering",
                    "AI-startup", "AI-industrien", "AI-marked", "AI-trends", "AI-fremtid", "robotter", "automatiserede systemer",
                    "intelligente maskiner", "AI-integration", "AI-implementering", "AI-optimering", "AI-overvågning"]

def is_ai_article(text):
    if not isinstance(text, str):
        return False
    t = text.lower()
    return any(k in t for k in AI_KEYWORDS)

df["is_ai"] = df["plain_text"].apply(is_ai_article)

df["is_ai"].value_counts()

df_ai = df[df["is_ai"]].copy()

print("AI-related articles:", df_ai.shape[0])

df_ai["published_date"] = pd.to_datetime(df_ai["published_date"], errors="coerce")
df_ai["year"] = df_ai["published_date"].dt.year

df_ai = df_ai[(df_ai["year"] >= 2016) & (df_ai["year"] <= 2024)]

df_ai["year"].value_counts().sort_index()

!pip install vaderSentiment -q

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import numpy as np

analyzer = SentimentIntensityAnalyzer()

def get_sentiment(text):
    if not isinstance(text, str) or len(text) < 50:
        return np.nan
    return analyzer.polarity_scores(text)["compound"]

df_ai["sentiment"] = df_ai["plain_text"].apply(get_sentiment)

df_ai["sentiment"].describe()

import matplotlib.pyplot as plt

ai_volume = (
    df_ai
    .groupby("year")
    .size()
    .reset_index(name="article_count")
)

plt.figure(figsize=(10,5))
plt.plot(ai_volume["year"], ai_volume["article_count"], marker="o")
plt.title("Volume of AI-Related Media Coverage in Denmark (2016–2024)")
plt.xlabel("Year")
plt.ylabel("Number of Articles")
plt.xticks(ai_volume["year"])
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

def sentiment_label(x):
    if x > 0.05:
        return "Positive"
    elif x < -0.05:
        return "Negative"
    else:
        return "Neutral"

df_ai["sentiment_label"] = df_ai["sentiment"].apply(sentiment_label)

sentiment_dist = (
    df_ai
    .groupby(["year", "sentiment_label"])
    .size()
    .reset_index(name="count")
)

pivot = sentiment_dist.pivot(
    index="year",
    columns="sentiment_label",
    values="count"
).fillna(0)

pivot.plot(kind="bar", stacked=True, figsize=(10,5))
plt.title("Sentiment Distribution of AI Coverage in Denmark (2016–2024)")
plt.xlabel("Year")
plt.ylabel("Number of Articles")
plt.tight_layout()
plt.show()

