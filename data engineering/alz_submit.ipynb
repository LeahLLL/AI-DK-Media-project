{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:52:00.545889Z",
     "start_time": "2025-12-17T06:52:00.541887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "IS_PRE = False"
   ],
   "id": "39268e0c53d31772",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:52:00.569059Z",
     "start_time": "2025-12-17T06:52:00.556980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if IS_PRE:\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\")\n",
    "    queries = [\n",
    "        \"Artiklen undersøger kunstig intelligens og machine learning.\",\n",
    "        \"Teksten handler om AI-politik, regulering og etik.\",\n",
    "        \"Dette dokument beskriver AI-teknologi, GPT og automatisering.\",\n",
    "        \"Artiklen diskuterer brugen af robotter og generativ AI.\"\n",
    "    ]\n",
    "\n",
    "    keywords_query = [\"AI\", \"kunstig intelligens\", \"maskinlæring\", \"dyb læring\", \"neural netværk\", \"automatisering\",\n",
    "                      \"robotik\", \"dataanalyse\", \"algoritme\", \"intelligente systemer\", \"GPT\", \"OPENAI\", \"LLM\", \"chatbot\",\n",
    "                      \"sprogmodel\", \"generativ AI\", \"AI-assistent\", \"AI-drevet\", \"computer vision\",\n",
    "                      \"naturlig sprogbehandling\",\n",
    "                      \"AI-platform\", \"AI-teknologi\", \"AI-forskning\", \"AI-innovation\", \"AI-applikationer\", \"AI-løsninger\",\n",
    "                      \"AI-udvikling\", \"AI-sikkerhed\", \"AI-etik\", \"AI-regulering\", \"AI-politik\", \"AI-strategi\",\n",
    "                      \"AI-investering\",\n",
    "                      \"AI-startup\", \"AI-industrien\", \"AI-marked\", \"AI-trends\", \"AI-fremtid\", \"robotter\",\n",
    "                      \"automatiserede systemer\",\n",
    "                      \"intelligente maskiner\", \"AI-integration\", \"AI-implementering\", \"AI-optimering\", \"AI-overvågning\", ]\n",
    "    import re\n",
    "\n",
    "    emb = np.load(\"results/01_embeddings.npy\")\n",
    "    df = pd.read_csv(\"results/00_raw_data.csv\")\n",
    "    ai_vec = model.encode(queries, normalize_embeddings=True)\n",
    "    query_vec = np.mean(ai_vec, axis=0)\n",
    "\n",
    "    true_score = util.cos_sim(emb, ai_vec)\n",
    "    ai_final_score = true_score.mean(dim=1)\n",
    "    # 1) 先把 keywords 分成 “短词” 和 “长词”\n",
    "    short_keywords = [kw for kw in keywords_query\n",
    "                      if len(kw) <= 3 and \" \" not in kw]\n",
    "    long_keywords = [kw for kw in keywords_query\n",
    "                     if kw not in short_keywords]\n",
    "\n",
    "    short_patterns = {\n",
    "        kw: re.compile(r\"\\b\" + re.escape(kw.lower()) + r\"\\b\", flags=re.IGNORECASE)\n",
    "        for kw in short_keywords\n",
    "    }\n",
    "\n",
    "    keyword_scores = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['plain_text']\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        score = 0\n",
    "        score += sum(\n",
    "            1 for kw in long_keywords\n",
    "            if kw.lower() in text_lower\n",
    "        )\n",
    "\n",
    "        score += sum(\n",
    "            1 for kw, pattern in short_patterns.items()\n",
    "            if pattern.search(text_lower)\n",
    "        )\n",
    "\n",
    "        keyword_scores.append(score)\n",
    "\n",
    "    keyword_scores = np.array(keyword_scores)\n",
    "    # assign to column\n",
    "    df['keyword_score'] = keyword_scores\n",
    "\n",
    "    df[\"ai_score\"] = ai_final_score\n",
    "\n",
    "    df[\"is_ai\"] = df[\"ai_score\"] > 0\n",
    "    # and keyword score > 0\n",
    "    df[\"is_ai\"] = df[\"is_ai\"] & (df[\"keyword_score\"] > 0)\n",
    "\n",
    "    ai_score_df = df"
   ],
   "id": "7fca10a327589008",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T06:52:07.449053Z",
     "start_time": "2025-12-17T06:52:07.327190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_1_v2.csv.tmp\")\n",
    "df2 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_2_v2.csv.tmp\")\n",
    "df3 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_3_v2.csv.tmp\")\n",
    "df4 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_0_v2.csv.tmp\")\n",
    "m_df1 = pd.read_csv(\"results/missing_shard_0.csv\")\n",
    "m_df2 = pd.read_csv(\"results/missing_shard_1.csv\")\n",
    "m_df3 = pd.read_csv(\"results/missing_shard_2.csv\")\n",
    "m_df4 = pd.read_csv(\"results/missing_shard_3.csv\")\n",
    "df = pd.concat([df1, df2, df3,df4,m_df1,m_df2,m_df3,m_df4], axis=0)\n",
    "print(df.shape)\n",
    "# df = pd.to_csv('dk_news_2016_2024_ai_shard_v2.csv')\n",
    "# drop duplicate orig_index row in df\n",
    "df.drop_duplicates(subset='orig_index',inplace=True)\n",
    "print(df.shape)\n",
    "if not IS_PRE:\n",
    "    ai_score_df = pd.read_csv(\"results/02_ai_scores_v2.csv\")\n",
    "# concat previous_df ['ai_score', 'keyword_score'] to df based on orig_index to previous_df index\n",
    "df = df.merge(ai_score_df[['ai_score', 'keyword_score']], left_on='orig_index', right_index=True, how='left')\n",
    "print(df.shape)\n",
    "df.dropna(inplace=True,subset=[\"is_ai_llm\"])\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.describe()\n",
    "# only keep year larger than 2015\n",
    "df = df[pd.to_datetime(df[\"published_date\"], errors='coerce').dt.year >= 2016]\n",
    "\n"
   ],
   "id": "670d332d29607fcc",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/dk_news_2016_2024_ai_shard_1_v2.csv.tmp'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df1 =  \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m./results/dk_news_2016_2024_ai_shard_1_v2.csv.tmp\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m df2 =  pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33m./results/dk_news_2016_2024_ai_shard_2_v2.csv.tmp\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m df3 =  pd.read_csv(\u001B[33m\"\u001B[39m\u001B[33m./results/dk_news_2016_2024_ai_shard_3_v2.csv.tmp\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Projects\\AI-DK-Media-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Projects\\AI-DK-Media-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Projects\\AI-DK-Media-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Projects\\AI-DK-Media-project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Projects\\AI-DK-Media-project\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: './results/dk_news_2016_2024_ai_shard_1_v2.csv.tmp'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# df1 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_1_v2.csv\")\n",
    "# df2 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_2_v2.csv\")\n",
    "# df3 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_3_v2.csv\")\n",
    "# df4 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_4_v2.csv\")  # <- small fix: shard_4, not 3\n",
    "# df = pd.concat([df1, df2, df3, df4], axis=0, ignore_index=True)\n",
    "\n",
    "# If not, uncomment and fix paths:\n",
    "# df1 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_1_v2.csv\")\n",
    "# df2 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_2_v2.csv\")\n",
    "# df3 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_3_v2.csv\")\n",
    "# df4 =  pd.read_csv(\"results/dk_news_2016_2024_ai_shard_4_v2.csv\")\n",
    "# df = pd.concat([df1, df2, df3, df4], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Directory for figures (PNG)\n",
    "FIG_DIR = \"figures_ai_llm_overview\"\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 1. Basic cleaning & helpers\n",
    "# =====================================================\n",
    "\n",
    "# 1.1 Parse dates and extract year\n",
    "if \"published_date\" in df.columns:\n",
    "    df[\"published_date\"] = pd.to_datetime(df[\"published_date\"], errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"published_date\"].dt.year\n",
    "else:\n",
    "    df[\"year\"] = np.nan\n",
    "\n",
    "\n",
    "def to_bool_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Robust boolean conversion:\n",
    "    - true:  True, 1, \"1\", \"true\", \"True\", \"ja\", \"yes\"\n",
    "    - false: False, 0, \"0\", \"false\", \"False\", \"nej\", \"no\"\n",
    "    - others: NaN\n",
    "    \"\"\"\n",
    "    s = series.astype(str).str.strip().str.lower()\n",
    "    true_vals = {\"1\", \"true\", \"ja\", \"yes\"}\n",
    "    false_vals = {\"0\", \"false\", \"nej\", \"no\"}\n",
    "\n",
    "    def _map_val(v: str):\n",
    "        if v in true_vals:\n",
    "            return True\n",
    "        if v in false_vals:\n",
    "            return False\n",
    "        return np.nan\n",
    "\n",
    "    return s.map(_map_val)\n",
    "\n",
    "\n",
    "# 1.2 Convert LLM AI flag to boolean\n",
    "if \"is_ai_llm\" in df.columns:\n",
    "    df[\"is_ai_llm_flag\"] = to_bool_series(df[\"is_ai_llm\"])\n",
    "else:\n",
    "    df[\"is_ai_llm_flag\"] = np.nan\n",
    "\n",
    "if \"is_ai\" in df.columns:\n",
    "    df[\"is_ai_flag\"] = to_bool_series(df[\"is_ai\"])\n",
    "else:\n",
    "    df[\"is_ai_flag\"] = np.nan\n",
    "\n",
    "# 1.3 Ensure relevance score is numeric\n",
    "if \"ai_relevance_llm\" in df.columns:\n",
    "    df[\"ai_relevance_llm\"] = pd.to_numeric(df[\"ai_relevance_llm\"], errors=\"coerce\")\n",
    "\n",
    "# 1.4 Ensure barrier flag is boolean\n",
    "if \"ai_barrier_llm\" in df.columns:\n",
    "    df[\"ai_barrier_llm_flag\"] = to_bool_series(df[\"ai_barrier_llm\"])\n",
    "else:\n",
    "    df[\"ai_barrier_llm_flag\"] = np.nan\n",
    "\n",
    "# =====================================================\n",
    "# 2. Basic descriptive statistics (printed)\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== Basic info ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== Missing value ratio per column ===\")\n",
    "missing_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "print(missing_ratio)\n",
    "\n",
    "print(\"\\n=== Numeric describe ===\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n=== Example topic / barrier columns head ===\")\n",
    "cols_preview = [\n",
    "    \"orig_index\",\n",
    "    \"published_date\",\n",
    "    \"sitename\",\n",
    "    \"publisher\",\n",
    "    \"is_ai_llm_flag\",\n",
    "    \"ai_relevance_llm\",\n",
    "    \"ai_topic_llm\",\n",
    "    \"ai_barrier_llm_flag\",\n",
    "    \"ai_barrier_type_llm\",\n",
    "]\n",
    "cols_preview = [c for c in cols_preview if c in df.columns]\n",
    "print(df[cols_preview].head(10))\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Plot helpers\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "def _save_show_close(fig: plt.Figure, filename: str, dpi: int = 200):\n",
    "    \"\"\"Save to FIG_DIR as PNG, show in notebook, then close to free memory.\"\"\"\n",
    "    out_path = os.path.join(FIG_DIR, filename)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. Distribution of AI relevance scores (hist)\n",
    "# =====================================================\n",
    "\n",
    "if \"ai_relevance_llm\" in df.columns:\n",
    "    x = df[\"ai_relevance_llm\"].dropna()\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.hist(x, bins=50)\n",
    "    ax.set_title(\"Distribution of AI Relevance (LLM Score 0–100)\")\n",
    "    ax.set_xlabel(\"AI Relevance (LLM, 0–100)\")\n",
    "    ax.set_ylabel(\"Number of Articles\")\n",
    "    _save_show_close(fig, \"01_ai_relevance_distribution.png\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Relationship: ai_score / keyword_score vs LLM relevance (scatter)\n",
    "# =====================================================\n",
    "\n",
    "MAX_POINTS_SCATTER = 50_000\n",
    "need_cols = [c for c in [\"ai_relevance_llm\", \"ai_score\", \"keyword_score\"] if c in df.columns]\n",
    "if \"ai_relevance_llm\" in need_cols:\n",
    "    scatter_df = df[need_cols].copy().dropna()\n",
    "    if len(scatter_df) > MAX_POINTS_SCATTER:\n",
    "        scatter_df = scatter_df.sample(MAX_POINTS_SCATTER, random_state=42)\n",
    "\n",
    "    if \"ai_score\" in scatter_df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(scatter_df[\"ai_score\"], scatter_df[\"ai_relevance_llm\"], s=6, alpha=0.4)\n",
    "        ax.set_title(\"SBERT AI Score vs LLM AI Relevance\")\n",
    "        ax.set_xlabel(\"ai_score (SBERT-based)\")\n",
    "        ax.set_ylabel(\"ai_relevance_llm (0–100)\")\n",
    "        _save_show_close(fig, \"02_sbert_vs_llm_relevance.png\")\n",
    "\n",
    "    if \"keyword_score\" in scatter_df.columns:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(scatter_df[\"keyword_score\"], scatter_df[\"ai_relevance_llm\"], s=6, alpha=0.4)\n",
    "        ax.set_title(\"Keyword Score vs LLM AI Relevance\")\n",
    "        ax.set_xlabel(\"keyword_score\")\n",
    "        ax.set_ylabel(\"ai_relevance_llm (0–100)\")\n",
    "        _save_show_close(fig, \"03_keyword_vs_llm_relevance.png\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. Yearly trends: total vs AI vs AI share\n",
    "# =====================================================\n",
    "\n",
    "if \"year\" in df.columns and \"orig_index\" in df.columns:\n",
    "    if \"is_ai_llm_flag\" in df.columns:\n",
    "        yearly = (\n",
    "            df.groupby(\"year\", dropna=True)\n",
    "            .agg(\n",
    "                total_articles=(\"orig_index\", \"count\"),\n",
    "                ai_articles=(\"is_ai_llm_flag\", lambda s: s.fillna(False).astype(int).sum()),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        yearly = df.groupby(\"year\", dropna=True).agg(total_articles=(\"orig_index\", \"count\")).reset_index()\n",
    "        yearly[\"ai_articles\"] = 0\n",
    "\n",
    "    yearly[\"ai_share\"] = np.where(\n",
    "        yearly[\"total_articles\"] > 0, yearly[\"ai_articles\"] / yearly[\"total_articles\"], np.nan\n",
    "    )\n",
    "\n",
    "    # 5.1 Total vs AI articles per year (grouped bars)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    years = yearly[\"year\"].astype(int).tolist()\n",
    "    x = np.arange(len(years))\n",
    "    width = 0.38\n",
    "    ax.bar(x - width / 2, yearly[\"total_articles\"], width=width, label=\"total_articles\")\n",
    "    ax.bar(x + width / 2, yearly[\"ai_articles\"], width=width, label=\"ai_articles\")\n",
    "    ax.set_title(\"Total vs AI-related Articles per Year (LLM labels)\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Number of Articles\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(years, rotation=0)\n",
    "    ax.legend()\n",
    "    _save_show_close(fig, \"04_yearly_total_vs_ai.png\")\n",
    "\n",
    "    # 5.2 AI share per year (line)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(yearly[\"year\"], yearly[\"ai_share\"], marker=\"o\")\n",
    "    ax.set_title(\"Share of AI-related Articles per Year (LLM labels)\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"AI Share (AI articles / all articles)\")\n",
    "    if np.isfinite(np.nanmax(yearly[\"ai_share\"])):\n",
    "        ax.set_ylim(0, max(0.05, np.nanmax(yearly[\"ai_share\"]) * 1.05))\n",
    "    _save_show_close(fig, \"05_yearly_ai_share.png\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. AI share per publisher\n",
    "# =====================================================\n",
    "\n",
    "if \"publisher\" in df.columns and \"orig_index\" in df.columns and \"is_ai_llm_flag\" in df.columns:\n",
    "    tmp = df[[\"publisher\", \"orig_index\", \"is_ai_llm_flag\"]].copy()\n",
    "    tmp[\"ai_int\"] = tmp[\"is_ai_llm_flag\"].fillna(False).astype(int)\n",
    "\n",
    "    publisher_stats = (\n",
    "        tmp.groupby(\"publisher\")\n",
    "        .agg(total_articles=(\"orig_index\", \"count\"), ai_articles=(\"ai_int\", \"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    publisher_stats[\"ai_share\"] = publisher_stats[\"ai_articles\"] / publisher_stats[\"total_articles\"]\n",
    "\n",
    "    MIN_ARTICLES = 500\n",
    "    publisher_stats = publisher_stats[publisher_stats[\"total_articles\"] >= MIN_ARTICLES]\n",
    "    publisher_stats = publisher_stats.sort_values(\"ai_share\", ascending=False).head(20)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(publisher_stats[\"publisher\"], publisher_stats[\"ai_share\"])\n",
    "    ax.set_title(\"Top Publishers by AI Share (LLM labels, min 500 articles)\")\n",
    "    ax.set_xlabel(\"Publisher\")\n",
    "    ax.set_ylabel(\"AI Share\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    _save_show_close(fig, \"06_publisher_ai_share.png\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. Topic distribution (ai_topic_llm)\n",
    "# =====================================================\n",
    "\n",
    "if \"ai_topic_llm\" in df.columns:\n",
    "    topic_counts = (\n",
    "        df[\"ai_topic_llm\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"\", np.nan)\n",
    "        .dropna()\n",
    "        .value_counts()\n",
    "        .reset_index()\n",
    "    )\n",
    "    topic_counts.columns = [\"ai_topic_llm\", \"count\"]\n",
    "    topic_counts = topic_counts[~topic_counts[\"ai_topic_llm\"].str.lower().isin([\"none\", \"nan\", \"other\"])]\n",
    "\n",
    "    TOP_N_TOPICS = 30\n",
    "    topic_counts_top = topic_counts.head(TOP_N_TOPICS)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.bar(topic_counts_top[\"ai_topic_llm\"], topic_counts_top[\"count\"])\n",
    "    ax.set_title(f\"Top {TOP_N_TOPICS} LLM Topic Labels\")\n",
    "    ax.set_xlabel(\"ai_topic_llm (free label)\")\n",
    "    ax.set_ylabel(\"Number of Articles\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    _save_show_close(fig, \"07_topic_distribution.png\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. Topic × Year heatmap\n",
    "# =====================================================\n",
    "\n",
    "if {\"ai_topic_llm\", \"year\", \"is_ai_llm_flag\", \"orig_index\"}.issubset(df.columns):\n",
    "    topic_year = (\n",
    "        df[df[\"is_ai_llm_flag\"] == True]\n",
    "        .groupby([\"year\", \"ai_topic_llm\"])\n",
    "        .agg(n=(\"orig_index\", \"count\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    top_topics = (\n",
    "        topic_year.groupby(\"ai_topic_llm\")[\"n\"].sum().sort_values(ascending=False).head(20).index.tolist()\n",
    "    )\n",
    "    topic_year = topic_year[topic_year[\"ai_topic_llm\"].isin(top_topics)]\n",
    "    heat = topic_year.pivot_table(index=\"year\", columns=\"ai_topic_llm\", values=\"n\", fill_value=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    im = ax.imshow(heat.values, aspect=\"auto\")\n",
    "    ax.set_title(\"AI-related Articles by Topic and Year (Top 20 topics)\")\n",
    "    ax.set_xlabel(\"Topic (ai_topic_llm)\")\n",
    "    ax.set_ylabel(\"Year\")\n",
    "    ax.set_xticks(np.arange(len(heat.columns)))\n",
    "    ax.set_xticklabels(heat.columns.tolist(), rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(np.arange(len(heat.index)))\n",
    "    ax.set_yticklabels(heat.index.astype(int).tolist())\n",
    "    fig.colorbar(im, ax=ax, label=\"Number of Articles\")\n",
    "    _save_show_close(fig, \"08_topic_year_heatmap.png\")\n",
    "\n",
    "# =====================================================\n",
    "# 9. Barrier analysis\n",
    "# =====================================================\n",
    "\n",
    "# 9.1 Barrier presence distribution\n",
    "if \"ai_barrier_llm_flag\" in df.columns:\n",
    "    barrier_counts = df[\"ai_barrier_llm_flag\"].value_counts(dropna=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.bar(barrier_counts.index.astype(str), barrier_counts.values)\n",
    "    ax.set_title(\"Distribution of AI Barrier Presence (LLM)\")\n",
    "    ax.set_xlabel(\"ai_barrier_llm_flag (True/False/NaN)\")\n",
    "    ax.set_ylabel(\"Number of Articles\")\n",
    "    _save_show_close(fig, \"09_barrier_presence_distribution.png\")\n",
    "\n",
    "# 9.2 Barrier type distribution (for AI-related + has_barrier)\n",
    "if {\"ai_barrier_type_llm\", \"is_ai_llm_flag\", \"ai_barrier_llm_flag\"}.issubset(df.columns):\n",
    "    barrier_type_df = df[(df[\"is_ai_llm_flag\"] == True) & (df[\"ai_barrier_llm_flag\"] == True)].copy()\n",
    "\n",
    "    barrier_type_counts = (\n",
    "        barrier_type_df[\"ai_barrier_type_llm\"]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .replace(\"\", np.nan)\n",
    "        .dropna()\n",
    "        .value_counts()\n",
    "        .reset_index()\n",
    "    )\n",
    "    barrier_type_counts.columns = [\"ai_barrier_type_llm\", \"count\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.bar(barrier_type_counts[\"ai_barrier_type_llm\"], barrier_type_counts[\"count\"])\n",
    "    ax.set_title(\"Barrier Types in AI-related Articles (LLM labels)\")\n",
    "    ax.set_xlabel(\"ai_barrier_type_llm (free label)\")\n",
    "    ax.set_ylabel(\"Number of Articles\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    _save_show_close(fig, \"10_barrier_type_distribution.png\")\n",
    "\n",
    "# 9.3 Barrier type × Year heatmap\n",
    "if {\"ai_barrier_type_llm\", \"year\", \"is_ai_llm_flag\", \"ai_barrier_llm_flag\", \"orig_index\"}.issubset(df.columns):\n",
    "    barrier_year = (\n",
    "        df[(df[\"is_ai_llm_flag\"] == True) & (df[\"ai_barrier_llm_flag\"] == True)]\n",
    "        .groupby([\"year\", \"ai_barrier_type_llm\"])\n",
    "        .agg(n=(\"orig_index\", \"count\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    top_barriers = (\n",
    "        barrier_year.groupby(\"ai_barrier_type_llm\")[\"n\"].sum().sort_values(ascending=False).head(20).index.tolist()\n",
    "    )\n",
    "    barrier_year = barrier_year[barrier_year[\"ai_barrier_type_llm\"].isin(top_barriers)]\n",
    "    heat_b = barrier_year.pivot_table(index=\"year\", columns=\"ai_barrier_type_llm\", values=\"n\", fill_value=0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    im = ax.imshow(heat_b.values, aspect=\"auto\")\n",
    "    ax.set_title(\"Barrier Types in AI-related Articles by Year (Top 20)\")\n",
    "    ax.set_xlabel(\"Barrier Type (ai_barrier_type_llm)\")\n",
    "    ax.set_ylabel(\"Year\")\n",
    "    ax.set_xticks(np.arange(len(heat_b.columns)))\n",
    "    ax.set_xticklabels(heat_b.columns.tolist(), rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(np.arange(len(heat_b.index)))\n",
    "    ax.set_yticklabels(heat_b.index.astype(int).tolist())\n",
    "    fig.colorbar(im, ax=ax, label=\"Number of Articles\")\n",
    "    _save_show_close(fig, \"11_barrier_type_year_heatmap.png\")\n",
    "\n",
    "print(\"\\nAll figures saved under:\", FIG_DIR)\n",
    "\n"
   ],
   "id": "b10bd4c99cfa8b4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# correlation matrix of is_ai_llm, ai_relevance_llm, ai_score, keyword_score, ai_barrier_llm_flag\n",
    "corr_cols = [\n",
    "    \"is_ai_llm\",\n",
    "    \"ai_relevance_llm\",\n",
    "    \"ai_score\",\n",
    "    \"keyword_score\",\n",
    "]\n",
    "corr_df = df[corr_cols].copy()\n",
    "corr_matrix = corr_df.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(FIG_DIR, \"12_correlation_matrix.png\"), bbox_inches=\"tight\")"
   ],
   "id": "4026c5c72db088d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# using umap and hdbscan to cluster articles based on their embeddings and see if clusters correspond to ai_llm labels\n",
    "from umap import UMAP\n",
    "ai_df = df[df['is_ai_llm']]\n",
    "\n",
    "emb = np.load(\"results/01_embeddings.npy\")\n",
    "# emb select by df orig_index\n",
    "emb = emb[ai_df[\"orig_index\"].values]\n",
    "\n",
    "reduced = UMAP(n_neighbors=15, min_dist=0.0, metric=\"cosine\").fit_transform(emb)\n",
    "import hdbscan\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=1000, metric='euclidean')\n",
    "labels = clusterer.fit_predict(reduced)\n",
    "ai_df[\"cluster\"] = labels\n"
   ],
   "id": "30a9112a0df5f095"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# Safety alignment\n",
    "# -----------------------\n",
    "ai_df = ai_df.copy().reset_index(drop=True)  # 重要：确保 reduced[i] 对应 ai_df.iloc[i]\n",
    "\n",
    "CLUSTER_COL = \"cluster\"\n",
    "TOPIC_COL = \"ai_topic_llm\"\n",
    "\n",
    "assert reduced.shape[0] == len(ai_df), \"reduced and ai_df length mismatch\"\n",
    "assert CLUSTER_COL in ai_df.columns, f\"missing {CLUSTER_COL}\"\n",
    "assert TOPIC_COL in ai_df.columns, f\"missing {TOPIC_COL}\"\n",
    "\n",
    "# -----------------------\n",
    "# Plot params\n",
    "# -----------------------\n",
    "MAX_PTS = 200_000     # 点太多就抽样（60k不用抽也行）\n",
    "RNG = 42\n",
    "LABEL_MAX_CHARS = 26  # 标签太长截断\n",
    "MIN_LABEL_N = 300     # 小簇不标注\n",
    "\n",
    "N = len(ai_df)\n",
    "idx = np.arange(N)\n",
    "if N > MAX_PTS:\n",
    "    idx = np.random.default_rng(RNG).choice(N, size=MAX_PTS, replace=False)\n",
    "\n",
    "x = reduced[idx, 0]\n",
    "y = reduced[idx, 1]\n",
    "clusters = ai_df.loc[idx, CLUSTER_COL].to_numpy()\n",
    "\n",
    "# unique clusters (keep -1 last)\n",
    "uniq = np.unique(ai_df[CLUSTER_COL].to_numpy())\n",
    "uniq = [c for c in uniq if c != -1] + ([-1] if -1 in uniq else [])\n",
    "\n",
    "# color map for non-noise clusters\n",
    "k = len([c for c in uniq if c != -1])\n",
    "palette = plt.cm.tab10(np.linspace(0, 1, max(k, 3)))\n",
    "color_map = {}\n",
    "pi = 0\n",
    "for c in uniq:\n",
    "    if c == -1:\n",
    "        color_map[c] = (0.6, 0.6, 0.6, 0.15)  # noise gray\n",
    "    else:\n",
    "        color_map[c] = palette[pi]\n",
    "        pi += 1\n",
    "\n",
    "# -----------------------\n",
    "# Compute cluster centers + majority ai_topic_llm (on FULL data, not just sampled)\n",
    "# -----------------------\n",
    "centers = {}\n",
    "labels_text = {}\n",
    "\n",
    "for c in uniq:\n",
    "    m_full = (ai_df[CLUSTER_COL].to_numpy() == c)\n",
    "    if m_full.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    cx = reduced[m_full, 0].mean()\n",
    "    cy = reduced[m_full, 1].mean()\n",
    "    centers[c] = (cx, cy)\n",
    "\n",
    "    # majority topic label\n",
    "    vc = ai_df.loc[m_full, TOPIC_COL].astype(str).value_counts()\n",
    "    major = vc.index[0] if len(vc) else \"\"\n",
    "    share = float(vc.iloc[0] / vc.sum()) if len(vc) else 0.0\n",
    "\n",
    "    # shorten\n",
    "    major2 = major.replace(\"\\n\", \" \").strip()\n",
    "    if len(major2) > LABEL_MAX_CHARS:\n",
    "        major2 = major2[:LABEL_MAX_CHARS] + \"…\"\n",
    "\n",
    "    labels_text[c] = f\"{major2} ({share:.0%})\"\n",
    "\n",
    "# -----------------------\n",
    "# Plot\n",
    "# -----------------------\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "\n",
    "for c in uniq:\n",
    "    m = (clusters == c)\n",
    "    if m.sum() == 0:\n",
    "        continue\n",
    "    ax.scatter(\n",
    "        x[m], y[m],\n",
    "        s=2,\n",
    "        alpha=0.35 if c != -1 else 0.10,\n",
    "        c=[color_map[c]],\n",
    "        label=f\"cluster {c} (n={int((ai_df[CLUSTER_COL]==c).sum())})\",\n",
    "        rasterized=True\n",
    "    )\n",
    "\n",
    "ax.set_title(\"UMAP + HDBSCAN clusters (colored by cluster) with center ai_topic_llm label\")\n",
    "ax.set_xlabel(\"UMAP-1\")\n",
    "ax.set_ylabel(\"UMAP-2\")\n",
    "ax.legend(loc=\"best\", fontsize=8, frameon=True)\n",
    "\n",
    "# Center labels\n",
    "for c in uniq:\n",
    "    n_c = int((ai_df[CLUSTER_COL] == c).sum())\n",
    "    if c == -1 or n_c < MIN_LABEL_N:\n",
    "        continue\n",
    "    cx, cy = centers[c]\n",
    "    ax.scatter([cx], [cy], marker=\"x\", s=120, linewidths=2)  # center marker\n",
    "    ax.text(\n",
    "        cx, cy,\n",
    "        labels_text[c],\n",
    "        fontsize=10, weight=\"bold\",\n",
    "        ha=\"center\", va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"none\", alpha=0.75)\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"umap_hdbscan_clusters_with_center_topic.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "# OUTPUT ALL cluster labels\n",
    "print(labels)\n"
   ],
   "id": "789e52a91cf8a028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "CLUSTER_COL = \"ai_relevance_llm\"        # 你已存在的聚类列\n",
    "TOPIC_COL   = \"ai_topic_llm\"   # 你要看的 topic 列（换成你的 topic 列名）\n",
    "TOPN = 30                      # 输出前 TOPN 个 topic\n",
    "\n",
    "# 1) 每个 cluster 的大小\n",
    "cluster_counts = ai_df[CLUSTER_COL].value_counts(dropna=False)\n",
    "print(\"Top clusters by size:\")\n",
    "print(cluster_counts.head(20))\n",
    "\n",
    "# 2) 最大 cluster（通常是 -1）\n",
    "n_l = 1\n",
    "largest_cluster = cluster_counts.index[n_l]\n",
    "largest_n = int(cluster_counts.iloc[n_l])\n",
    "print(f\"\\nLargest cluster = {largest_cluster}, n = {largest_n}\")\n",
    "\n",
    "# 3) 最大 cluster 内部的 topic 分布\n",
    "sub = ai_df[ai_df[CLUSTER_COL] == largest_cluster].copy()\n",
    "\n",
    "topic_dist = (\n",
    "    sub[TOPIC_COL]\n",
    "    .fillna(\"NA\")\n",
    "    .astype(str)\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    ")\n",
    "topic_dist.columns = [TOPIC_COL, \"count\"]\n",
    "topic_dist[\"share\"] = topic_dist[\"count\"] / topic_dist[\"count\"].sum()\n",
    "\n",
    "print(f\"\\nTopic distribution inside cluster {largest_cluster} (top {TOPN}):\")\n",
    "print(topic_dist.head(TOPN))\n",
    "\n",
    "# 可选：保存\n",
    "topic_dist.to_csv(f\"results/topic_dist_cluster_{largest_cluster}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nSaved: results/topic_dist_cluster_{largest_cluster}.csv\")\n"
   ],
   "id": "76dde2671b4f97db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "count = df['ai_topic_llm'].str.contains('AI', case=False, na=False).sum()\n",
    "print(count)"
   ],
   "id": "f9e971c65e4ceabd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.columns",
   "id": "75fae32086501063"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "RNG = 42\n",
    "SAMPLE_N = 200_000          # elbow computed on sample (increase if you can)\n",
    "USE_PCA = True\n",
    "PCA_DIM = 50                # good default for SBERT embeddings\n",
    "K_LIST = list(range(1, 50, 1))  # try 20..500 step 20; adjust for your scale\n",
    "BATCH_SIZE = 8192\n",
    "\n",
    "FINAL_COL = \"cluster_kmeans\"\n",
    "\n",
    "assert len(ai_df) == emb.shape[0], \"ai_df and emb must be aligned (same N)\"\n",
    "ai_df = ai_df.reset_index(drop=True)\n",
    "\n",
    "# =========================\n",
    "# 1) Normalize embeddings (cosine-friendly)\n",
    "# =========================\n",
    "emb = np.asarray(emb, dtype=np.float32)\n",
    "emb_norm = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "N = emb_norm.shape[0]\n",
    "n_s = min(SAMPLE_N, N)\n",
    "idx = np.random.default_rng(RNG).choice(N, size=n_s, replace=False)\n",
    "X_s = emb_norm[idx]\n",
    "\n",
    "# Optional PCA for speed/stability\n",
    "if USE_PCA:\n",
    "    print(f\"[INFO] PCA to {PCA_DIM} dims on sample...\")\n",
    "    pca = PCA(n_components=PCA_DIM, random_state=RNG)\n",
    "    X_s_pca = pca.fit_transform(X_s)\n",
    "else:\n",
    "    pca = None\n",
    "    X_s_pca = X_s\n",
    "\n",
    "# =========================\n",
    "# 2) Elbow curve (inertia vs k)\n",
    "# =========================\n",
    "inertias = []\n",
    "print(\"[INFO] Computing inertia for K:\", K_LIST)\n",
    "for k in K_LIST:\n",
    "    km = MiniBatchKMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=RNG,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_init=\"auto\"\n",
    "    )\n",
    "    km.fit(X_s_pca)\n",
    "    inertias.append(float(km.inertia_))\n",
    "    print(f\"  k={k:4d} inertia={inertias[-1]:.3e}\")\n",
    "\n",
    "# =========================\n",
    "# 3) Auto elbow detection (max distance to line)\n",
    "#    Works well on log(inertia).\n",
    "# =========================\n",
    "ks = np.array(K_LIST, dtype=float)\n",
    "ys = np.log(np.array(inertias, dtype=float))\n",
    "\n",
    "# line between first and last point\n",
    "p1 = np.array([ks[0], ys[0]])\n",
    "p2 = np.array([ks[-1], ys[-1]])\n",
    "\n",
    "# distance from each point to the line (normalized)\n",
    "# formula: |(p2-p1)x(p1-p)| / ||p2-p1||\n",
    "v = p2 - p1\n",
    "v_norm = np.linalg.norm(v) + 1e-12\n",
    "\n",
    "dists = []\n",
    "for k, y in zip(ks, ys):\n",
    "    p = np.array([k, y])\n",
    "    # 2D cross product magnitude\n",
    "    cross = abs(v[0]*(p1[1]-p[1]) - v[1]*(p1[0]-p[0]))\n",
    "    dists.append(cross / v_norm)\n",
    "\n",
    "dists = np.array(dists)\n",
    "best_i = int(np.argmax(dists))\n",
    "best_k = int(ks[best_i])\n",
    "\n",
    "print(f\"\\n[OK] Auto-elbow K = {best_k} (index={best_i})\")\n",
    "\n",
    "# Plot elbow\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(K_LIST, inertias, marker=\"o\")\n",
    "plt.title(\"Elbow Method (MiniBatchKMeans on sample)\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia (SSE)\")\n",
    "plt.grid(True)\n",
    "plt.axvline(best_k, linestyle=\"--\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kmeans_elbow.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"[OK] Saved plot: kmeans_elbow.png\")\n",
    "\n",
    "# =========================\n",
    "# 4) Fit FINAL KMeans on ALL data\n",
    "# =========================\n",
    "if USE_PCA:\n",
    "    print(\"[INFO] Transform full embeddings with PCA...\")\n",
    "    X_full = pca.transform(emb_norm)\n",
    "else:\n",
    "    X_full = emb_norm\n",
    "\n",
    "print(f\"[INFO] Fitting final MiniBatchKMeans with k={best_k} on full data (N={N})...\")\n",
    "final_km = MiniBatchKMeans(\n",
    "    n_clusters=best_k,\n",
    "    random_state=RNG,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_init=\"auto\"\n",
    ")\n",
    "labels_full = final_km.fit_predict(X_full)\n",
    "\n",
    "ai_df[FINAL_COL] = labels_full\n",
    "\n",
    "# Cluster size summary\n",
    "sizes = ai_df[FINAL_COL].value_counts().sort_values(ascending=False)\n",
    "print(\"\\nTop cluster sizes:\")\n",
    "print(sizes.head(20))\n",
    "print(\"\\nSmallest cluster sizes:\")\n",
    "print(sizes.tail(20))\n",
    "\n",
    "# Optional: save\n",
    "ai_df[[FINAL_COL]].to_csv(\"results/kmeans_cluster_labels.csv\", index=False)\n",
    "print(\"\\n[OK] Saved labels: results/kmeans_cluster_labels.csv\")\n"
   ],
   "id": "e7461958b8b8a69f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------\n",
    "# CONFIG\n",
    "# -------------------\n",
    "CLUSTER_COL = \"cluster_kmeans\"\n",
    "TOPIC_COL   = \"ai_topic_llm\"\n",
    "\n",
    "MAX_PTS = 200_000      # 散点抽样数（太大容易卡）\n",
    "TOPK_PLOT = 30         # 只画最大的 Top-K 个簇（其余不画）——强烈推荐，否则太乱\n",
    "TOPK_LABEL = 30        # 标注多少个簇的中心（建议与 TOPK_PLOT 一致）\n",
    "MIN_LABEL_N = 200      # 小簇不标注（避免文字挤爆）\n",
    "LABEL_MAX_CHARS = 22   # 标注文本截断长度\n",
    "\n",
    "RNG = 42\n",
    "\n",
    "# -------------------\n",
    "# sanity\n",
    "# -------------------\n",
    "assert len(ai_df) == reduced.shape[0], \"ai_df and reduced must align\"\n",
    "assert CLUSTER_COL in ai_df.columns, f\"Missing {CLUSTER_COL}\"\n",
    "assert TOPIC_COL in ai_df.columns, f\"Missing {TOPIC_COL}\"\n",
    "\n",
    "ai_df = ai_df.reset_index(drop=True)\n",
    "\n",
    "# -------------------\n",
    "# 1) pick Top-K clusters to plot (optional but recommended)\n",
    "# -------------------\n",
    "sizes = ai_df[CLUSTER_COL].value_counts()\n",
    "top_clusters = sizes.head(TOPK_PLOT).index.to_numpy()\n",
    "\n",
    "mask_top = ai_df[CLUSTER_COL].isin(top_clusters).to_numpy()\n",
    "idx_all = np.where(mask_top)[0]\n",
    "print(f\"Plotting Top-{TOPK_PLOT} clusters: {len(idx_all)} / {len(ai_df)} ({len(idx_all)/len(ai_df):.2%})\")\n",
    "\n",
    "# sample only from selected clusters\n",
    "idx = np.random.default_rng(RNG).choice(idx_all, size=min(MAX_PTS, len(idx_all)), replace=False)\n",
    "\n",
    "x = reduced[idx, 0]\n",
    "y = reduced[idx, 1]\n",
    "c = ai_df.loc[idx, CLUSTER_COL].to_numpy()\n",
    "\n",
    "# -------------------\n",
    "# 2) compute cluster centers on FULL selected data (not just sample)\n",
    "# -------------------\n",
    "sub = ai_df.loc[idx_all, [CLUSTER_COL, TOPIC_COL]].copy()\n",
    "sub[\"x\"] = reduced[idx_all, 0]\n",
    "sub[\"y\"] = reduced[idx_all, 1]\n",
    "\n",
    "# center (mean in reduced space)\n",
    "centers = sub.groupby(CLUSTER_COL)[[\"x\", \"y\"]].mean()\n",
    "\n",
    "# majority ai_topic_llm per cluster (mode)\n",
    "major_topic = (\n",
    "    sub.groupby(CLUSTER_COL)[TOPIC_COL]\n",
    "       .agg(lambda s: s.astype(str).value_counts().idxmax() if len(s) else \"\")\n",
    ")\n",
    "\n",
    "center_df = centers.join(major_topic.rename(\"major_topic\"))\n",
    "center_df[\"n_docs\"] = sizes.loc[center_df.index].astype(int).values\n",
    "center_df = center_df.sort_values(\"n_docs\", ascending=False)\n",
    "\n",
    "# choose which clusters to label\n",
    "label_clusters = center_df.head(TOPK_LABEL).index.to_numpy()\n",
    "\n",
    "# -------------------\n",
    "# 3) plot\n",
    "# -------------------\n",
    "fig, ax = plt.subplots(figsize=(11, 8))\n",
    "\n",
    "# Use categorical codes for coloring (works even if cluster ids are big numbers)\n",
    "cat = pd.Series(c).astype(\"category\")\n",
    "codes = cat.cat.codes.to_numpy()\n",
    "cats  = cat.cat.categories.to_numpy()\n",
    "\n",
    "# discrete colormap with enough bins for TOPK_PLOT\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(cats)) if len(cats) <= 20 else plt.cm.get_cmap(\"gist_ncar\", len(cats))\n",
    "\n",
    "sc = ax.scatter(\n",
    "    x, y,\n",
    "    c=codes,\n",
    "    s=2,\n",
    "    alpha=0.35,\n",
    "    cmap=cmap,\n",
    "    rasterized=True\n",
    ")\n",
    "\n",
    "ax.set_title(f\"UMAP colored by {CLUSTER_COL} (Top-{TOPK_PLOT}); centers labeled by majority {TOPIC_COL}\")\n",
    "ax.set_xlabel(\"UMAP-1\")\n",
    "ax.set_ylabel(\"UMAP-2\")\n",
    "\n",
    "# plot centers + labels\n",
    "for cid in label_clusters:\n",
    "    row = center_df.loc[cid]\n",
    "    if row[\"n_docs\"] < MIN_LABEL_N:\n",
    "        continue\n",
    "\n",
    "    cx, cy = row[\"x\"], row[\"y\"]\n",
    "    topic = str(row[\"major_topic\"]).strip().replace(\"\\n\", \" \")\n",
    "    if len(topic) > LABEL_MAX_CHARS:\n",
    "        topic = topic[:LABEL_MAX_CHARS] + \"…\"\n",
    "\n",
    "    ax.scatter([cx], [cy], marker=\"x\", s=80, linewidths=2)  # center mark\n",
    "    ax.text(\n",
    "        cx, cy,\n",
    "        f\"{topic}\",\n",
    "        fontsize=9,\n",
    "        ha=\"center\", va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"none\", alpha=0.7)\n",
    "    )\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"umap_kmeans_clusters_labeled_by_ai_topic_llm.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "id": "ee641825fa76b93e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.unique(labels)",
   "id": "b77e07b60411651d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1) 先写入数值簇标签\n",
    "ai_df[\"cluster\"] = labels\n",
    "\n",
    "# 2) 计算每个簇里出现最多的 ai_topic_llm（这就是你图里 labels_text 的逻辑）\n",
    "cluster_to_topic = (\n",
    "    ai_df.groupby(\"cluster\")[\"ai_topic_llm\"]\n",
    "        .agg(lambda s: s.fillna(\"NA\").astype(str).value_counts().idxmax())\n",
    ")\n",
    "\n",
    "# 3) 用这个“中心 topic”替换 cluster 列\n",
    "ai_df[\"cluster\"] = ai_df[\"cluster\"].map(cluster_to_topic)\n",
    "\n",
    "# 4) （可选）如果你想让噪声保持为 -1，而不是被替换成某个 topic，就加这一行\n",
    "#     放在 map 之后，因为 map 会把 -1 也替换掉\n",
    "ai_df.loc[labels == -1, \"cluster\"] = -1\n"
   ],
   "id": "2a8a69d523ce3906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cluster_to_topic\n",
    "# ouput a random plain text from cluster 1\n"
   ],
   "id": "cc1e31b8406a4d0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CLUSTER_COL = \"cluster\"\n",
    "TEXT_COL = \"plain_text\"\n",
    "TITLE_COL = \"title\"            # 没有就删掉\n",
    "DATE_COL = \"published_date\"    # 没有就删掉\n",
    "\n",
    "TARGET_CLUSTER = 'AI policy'          # <<< 改这里：例如 -1 或 \"policy\" 等\n",
    "condition = ai_df['keyword_score']>1\n",
    "sub = ai_df[(ai_df[CLUSTER_COL] == TARGET_CLUSTER)&condition]\n",
    "print(\"cluster =\", TARGET_CLUSTER, \"n =\", len(sub))\n",
    "\n",
    "if len(sub) == 0:\n",
    "    print(\"No rows in this cluster.\")\n",
    "else:\n",
    "    i = np.random.default_rng(np.random.randint(0,100)).choice(sub.index, size=1)[0]\n",
    "    row = ai_df.loc[i]\n",
    "\n",
    "    # 可选：打印元信息\n",
    "    if TITLE_COL in ai_df.columns:\n",
    "        print(\"\\nTITLE:\", row[TITLE_COL])\n",
    "    if DATE_COL in ai_df.columns:\n",
    "        print(\"DATE :\", row[DATE_COL])\n",
    "    print(\"INDEX:\", i)\n",
    "\n",
    "    # 输出正文\n",
    "    print(\"\\nPLAIN_TEXT:\\n\")\n",
    "    print(str(row[TEXT_COL]))\n",
    "    print(row)"
   ],
   "id": "4844a214df1e91d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 列名里有空格的话用这种写法\n",
    "CLUSTER_COL = \"cluster\"\n",
    "KEYWORD_COL = \"keyword_score\"   # <<< 这里换成你真实的 keyword score 列名（看你写法像有空格）\n",
    "\n",
    "# 1) 每个 cluster 的 keyword_score 均值\n",
    "cluster_kw = (\n",
    "    ai_df.groupby(CLUSTER_COL)[KEYWORD_COL]\n",
    "         .mean()\n",
    "         .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# 2) 最大均值的 cluster\n",
    "best_cluster = cluster_kw.index[0]\n",
    "best_mean = cluster_kw.iloc[0]\n",
    "\n",
    "print(\"Best cluster by mean keyword score:\")\n",
    "print(\"cluster =\", best_cluster, \"mean_keyword_score =\", best_mean)\n",
    "\n",
    "# 3) 可选：把 top 20 打出来\n",
    "print(\"\\nTop 20 clusters:\")\n",
    "print(cluster_kw.head(20))\n"
   ],
   "id": "9ea80beaa38bad8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 列名里有空格的话用这种写法\n",
    "CLUSTER_COL = \"cluster\"\n",
    "KEYWORD_COL = \"keyword_score\"   # <<< 这里换成你真实的 keyword score 列名（看你写法像有空格）\n",
    "\n",
    "# 1) 每个 cluster 的 keyword_score 均值\n",
    "cluster_kw = (\n",
    "    ai_df.groupby(CLUSTER_COL)[KEYWORD_COL]\n",
    "         .mean()\n",
    "         .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# 2) 最大均值的 cluster\n",
    "best_cluster = cluster_kw.index[0]\n",
    "best_mean = cluster_kw.iloc[0]\n",
    "\n",
    "print(\"Best cluster by mean keyword score:\")\n",
    "print(\"cluster =\", best_cluster, \"mean_keyword_score =\", best_mean)\n",
    "\n",
    "# 3) 可选：把 top 20 打出来\n",
    "print(\"\\nTop 20 clusters:\")\n",
    "print(cluster_kw)\n",
    "\n",
    "# output total rows that cluster by key_score>0.014\n",
    "target_cluster = [x for x in cluster_kw.index if cluster_kw[x]>0.014]\n",
    "print(target_cluster)\n",
    "# locate rows in ai_df that belong to target_cluster\n",
    "sub = ai_df[ai_df['cluster'].isin(target_cluster)]\n",
    "sub.shape\n",
    "\n",
    "\n"
   ],
   "id": "5c44dbe5380b5b04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "CLUSTER_COL = \"cluster\"\n",
    "KEYWORD_COL = \"ai_score\"   # <<< 这里换成你真实的 keyword score 列名（看你写法像有空格）\n",
    "\n",
    "# 1) 每个 cluster 的 keyword_score 均值\n",
    "cluster_kw = (\n",
    "    sub.groupby(CLUSTER_COL)[KEYWORD_COL]\n",
    "         .mean()\n",
    "         .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# 2) 最大均值的 cluster\n",
    "best_cluster = cluster_kw.index[0]\n",
    "best_mean = cluster_kw.iloc[0]\n",
    "\n",
    "print(\"Best cluster by mean embedding similarity score :\")\n",
    "print(\"cluster =\", best_cluster, \"mean_similarity_score =\", best_mean)\n",
    "\n",
    "# 3) 可选：把 top 20 打出来\n",
    "print(\"\\nTop 20 clusters:\")\n",
    "print(cluster_kw)\n",
    "\n",
    "# output total rows that cluster by key_score>0.014\n",
    "target_cluster = [x for x in cluster_kw.index if cluster_kw[x]>0.267]\n",
    "print(target_cluster)\n",
    "# locate rows in ai_df that belong to target_cluster\n",
    "sub2 = ai_df[ai_df['cluster'].isin(target_cluster)]\n",
    "sub2.shape"
   ],
   "id": "1621c073822144f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# keep only ai relative cluster\n",
    "ai_relative_cluster = [\n",
    "    \"automation\",\n",
    "    \"digital transformation\",\n",
    "    \"AI policy\",\n",
    "    \"electric vehicle launch\",\n",
    "    \"AI in healthcare\",\n",
    "    \"AI in politics\",\n",
    "    \"AI product launch\",\n",
    "]\n",
    "tmp_sb = sub2[sub2[\"cluster\"].isin(ai_relative_cluster)].copy()\n",
    "\n",
    "# ai_topic_llm distribution (Top 30)\n",
    "tmp_topics = tmp_sb[\"ai_topic_llm\"].value_counts().head(30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(tmp_topics.values, marker=\"o\")\n",
    "ax.set_xticks(range(len(tmp_topics)))\n",
    "ax.set_xticklabels(tmp_topics.index, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_title(\"Top 30 LLM AI Topics in Selected AI-Related Clusters\")\n",
    "ax.set_xlabel(\"ai_topic_llm (Top 30)\")\n",
    "ax.set_ylabel(\"Number of Articles\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "5944f0f08749f5b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tmp_topics",
   "id": "496ec765481d213e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# read embedding then select\n",
    "target_df = tmp_sb\n",
    "emb = np.load(\"results/01_embeddings.npy\")\n",
    "# emb select by index base on sub2 orig_index column\n",
    "emb = emb[tmp_sb['orig_index']]"
   ],
   "id": "276189ac5c1319f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "barrier_queries = {\n",
    "    \"skills_gap\": (\n",
    "        \"mangel på AI-kompetencer, mangel på tekniske færdigheder, \"\n",
    "        \"utilstrækkelig ekspertise i data og machine learning, \"\n",
    "        \"svært ved at rekruttere specialister, behov for efteruddannelse i AI\"\n",
    "    ),\n",
    "    \"resistance\": (\n",
    "        \"medarbejdermodstand mod AI, frygt for automatisering, \"\n",
    "        \"bekymring for at miste jobbet, modvilje mod digitale forandringer, \"\n",
    "        \"kulturel modstand i organisationen\"\n",
    "    ),\n",
    "    \"privacy\": (\n",
    "        \"databeskyttelse og privatliv, GDPR-bekymringer, \"\n",
    "        \"persondata og følsomme oplysninger, krav til samtykke og lovlig behandling, \"\n",
    "        \"risiko for datalæk og misbrug af personoplysninger\"\n",
    "    ),\n",
    "    \"data_quality\": (\n",
    "        \"dårlig datakvalitet, ufuldstændige eller manglende data, \"\n",
    "        \"inkonsistente datasæt, støj og fejl i registreringer, \"\n",
    "        \"manglende datastandarder og governance\"\n",
    "    ),\n",
    "    \"ethics_bias\": (\n",
    "        \"etiske problemer ved AI, algoritmisk bias, skævheder i data, \"\n",
    "        \"u-retfærdig eller diskriminerende adfærd, \"\n",
    "        \"bekymringer om ligebehandling og fairness\"\n",
    "    ),\n",
    "    \"transparency\": (\n",
    "        \"manglende gennemsigtighed i modeller, black-box AI-systemer, \"\n",
    "        \"svært at forklare beslutninger, mangel på forklarlighed og traceability, \"\n",
    "        \"krav om dokumentation og audit af AI-løsninger\"\n",
    "    ),\n",
    "}\n"
   ],
   "id": "f290a39ac17b2901"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "sbert search for barrier type",
   "id": "4a9f42fac9133c4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "barrier_texts = list(barrier_queries.values())\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "barrier_emb = model.encode(barrier_texts, normalize_embeddings=True)\n",
    "\n",
    "# Compute similarity\n",
    "barrier_scores = util.cos_sim(emb, barrier_emb).cpu().numpy()\n",
    "\n",
    "# Assign label with maximum similarity, with tqdm progress bar\n",
    "b_type = []\n",
    "argmax_indices = barrier_scores.argmax(axis=1)\n",
    "keys = list(barrier_queries.keys())\n",
    "for idx in tqdm(argmax_indices, desc=\"Classifying Barriers\", total=barrier_scores.shape[0]):\n",
    "    b_type.append(keys[idx])\n",
    "\n",
    "target_df[\"cos_barrier_type\"] = b_type\n"
   ],
   "id": "1ae0b8e459a1693f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# distribution of cos_barrier_type\n",
    "\n",
    "# ai_topic_llm distribution (Top 30)\n",
    "tmp_bars = target_df[\"cos_barrier_type\"].value_counts().head(30)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(tmp_bars.values, marker=\"o\")\n",
    "ax.set_xticks(range(len(tmp_bars)))\n",
    "ax.set_xticklabels(tmp_bars.index, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_title(\"Top 30 SBERT Cosine-Similarity Barriers in Selected AI-Related Clusters\")\n",
    "ax.set_xlabel(\"cos_barrier_type (Top 30)\")\n",
    "ax.set_ylabel(\"Number of Articles\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "4fdcd5e5eac819ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(target_df.shape)\n",
    "target_df['ai_barrier_type_llm'].value_counts()\n",
    "print(5000/7)\n"
   ],
   "id": "b711bba1dd6e57d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "BARRIER_COL = \"ai_barrier_type_llm\"   # free-text barrier label from LLM\n",
    "\n",
    "labels_series = (\n",
    "    target_df[BARRIER_COL]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "# Define \"no barrier\" variants\n",
    "no_barrier_tokens = {\n",
    "    \"none\", # that's for sure\n",
    "}\n",
    "\n",
    "is_no_barrier = labels_series.isin(no_barrier_tokens)\n",
    "\n",
    "# Keep only real barrier descriptions for clustering\n",
    "labels_for_clustering = labels_series[~is_no_barrier].dropna()\n",
    "labels_for_clustering = labels_for_clustering[labels_for_clustering != \"\"]\n",
    "\n",
    "# prepare unique label\n",
    "unique_labels = labels_for_clustering.unique().tolist()\n",
    "# value counts\n",
    "global_freq = labels_for_clustering.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "embeddings = model.encode(\n",
    "    unique_labels,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# ======================================\n",
    "# 3. UMAP dimensionality reduction\n",
    "#    (for clustering, not just visualization)\n",
    "# ======================================\n",
    "\n",
    "reducer = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "embeddings_umap = reducer.fit_transform(embeddings)\n",
    "print(\"Reduced embedding shape:\", embeddings_umap.shape)\n",
    "\n",
    "# ======================================\n",
    "# 4. HDBSCAN clustering on UMAP space\n",
    "# ======================================\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=20,         # tune: larger → fewer, more stable clusters\n",
    "    min_samples=5,               # tune: noise vs tightness\n",
    "    metric=\"euclidean\",          # UMAP output is in Euclidean space\n",
    "    cluster_selection_method=\"eom\",\n",
    ")\n",
    "\n",
    "cluster_ids = clusterer.fit_predict(embeddings_umap)\n",
    "\n",
    "n_clusters = len(set(cluster_ids)) - (1 if -1 in cluster_ids else 0)\n",
    "print(f\"HDBSCAN clusters (excluding noise): {n_clusters}\")\n",
    "print(\"Cluster label counts:\", np.unique(cluster_ids, return_counts=True))\n",
    "\n",
    "cluster_df = pd.DataFrame({\n",
    "    \"raw_label\": unique_labels,\n",
    "    \"cluster_id\": cluster_ids,\n",
    "})\n",
    "\n",
    "# ======================================\n",
    "# 5. Choose representative label per cluster\n",
    "# ======================================\n",
    "\n",
    "def pick_representative_label(group: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Pick a canonical label for a cluster:\n",
    "    - highest global frequency first\n",
    "    - then lexicographically\n",
    "    \"\"\"\n",
    "    g = group.copy()\n",
    "    g[\"freq\"] = g[\"raw_label\"].map(global_freq).fillna(0)\n",
    "    g = g.sort_values([\"freq\", \"raw_label\"], ascending=[False, True])\n",
    "    return g[\"raw_label\"].iloc[0]\n",
    "\n",
    "# Only for non-noise clusters (cluster_id != -1)\n",
    "valid_clusters = cluster_df[cluster_df[\"cluster_id\"] != -1]\n",
    "\n",
    "cluster_repr = (\n",
    "    valid_clusters\n",
    "    .groupby(\"cluster_id\")\n",
    "    .apply(pick_representative_label)\n",
    "    .rename(\"cluster_label\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge representative labels back\n",
    "cluster_df = cluster_df.merge(cluster_repr, on=\"cluster_id\", how=\"left\")\n",
    "\n",
    "# Noise clusters (cluster_id == -1) get label \"other\"\n",
    "cluster_df.loc[cluster_df[\"cluster_id\"] == -1, \"cluster_label\"] = \"other\"\n",
    "\n",
    "# Build mapping dicts: raw_label -> cluster_id / cluster_label\n",
    "raw_to_cluster_id = dict(zip(cluster_df[\"raw_label\"], cluster_df[\"cluster_id\"]))\n",
    "raw_to_cluster_label = dict(zip(cluster_df[\"raw_label\"], cluster_df[\"cluster_label\"]))\n",
    "\n",
    "# ======================================\n",
    "# 6. Map back to full df\n",
    "# ======================================\n",
    "\n",
    "target_df[\"barrier_label_raw\"] = labels_series  # cleaned original\n",
    "\n",
    "NO_BARRIER_CLUSTER_ID = -999\n",
    "NO_BARRIER_CLUSTER_LABEL = \"none\"\n",
    "\n",
    "def map_cluster_id(label: str):\n",
    "    label = str(label).strip().lower()\n",
    "    if label in no_barrier_tokens:\n",
    "        return NO_BARRIER_CLUSTER_ID\n",
    "    return raw_to_cluster_id.get(label, -1)  # -1 = HDBSCAN noise / unknown\n",
    "\n",
    "def map_cluster_label(label: str):\n",
    "    label = str(label).strip().lower()\n",
    "    if label in no_barrier_tokens:\n",
    "        return NO_BARRIER_CLUSTER_LABEL\n",
    "    return raw_to_cluster_label.get(label, \"other\")\n",
    "\n",
    "target_df[\"barrier_cluster_id\"] = target_df[\"barrier_label_raw\"].map(map_cluster_id)\n",
    "target_df[\"barrier_cluster_label\"] = target_df[\"barrier_label_raw\"].map(map_cluster_label)\n",
    "\n",
    "print(\n",
    "    target_df[\n",
    "        [\"ai_barrier_type_llm\",\n",
    "         \"barrier_label_raw\",\n",
    "         \"barrier_cluster_id\",\n",
    "         \"barrier_cluster_label\"]\n",
    "    ].head(20)\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# 7. (Optional) Save cluster mapping for inspection\n",
    "# ======================================\n",
    "\n",
    "out_dir = \"results\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "cluster_dict_path = os.path.join(out_dir, \"barrier_label_clusters_umap_hdbscan.csv\")\n",
    "cluster_df.to_csv(cluster_dict_path, index=False)\n",
    "print(\"Saved barrier label cluster mapping to:\", cluster_dict_path)\n"
   ],
   "id": "47e4fbfade53c27d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 2D UMAP for visualization\n",
    "umap_vis = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=2,\n",
    "    min_dist=0.05,\n",
    "    metric=\"cosine\",\n",
    "    random_state=42,\n",
    ")\n",
    "embeddings_2d = umap_vis.fit_transform(embeddings)  # shape (n_labels, 2)\n",
    "\n",
    "# Attach coordinates to cluster_df\n",
    "cluster_df[\"x\"] = embeddings_2d[:, 0]\n",
    "cluster_df[\"y\"] = embeddings_2d[:, 1]\n",
    "\n",
    "OUT_DIR = \"figures_barrier_clusters\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Option A: color by numeric cluster_id\n",
    "fig = px.scatter(\n",
    "    cluster_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster_id\",\n",
    "    hover_data={\n",
    "        \"raw_label\": True,\n",
    "        \"cluster_id\": True,\n",
    "    },\n",
    "    title=\"Barrier label clusters (UMAP 2D + HDBSCAN, colored by cluster_id)\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=7, opacity=0.8))\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(OUT_DIR, \"barrier_clusters_by_id.html\"))\n",
    "\n",
    "# Option B: if you also have a human-readable cluster_label\n",
    "if \"cluster_label\" in cluster_df.columns:\n",
    "    fig2 = px.scatter(\n",
    "        cluster_df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"cluster_label\",\n",
    "        hover_data={\n",
    "            \"raw_label\": True,\n",
    "            \"cluster_id\": True,\n",
    "            \"cluster_label\": True,\n",
    "        },\n",
    "        title=\"Barrier label clusters (UMAP 2D + HDBSCAN, colored by cluster_label)\",\n",
    "    )\n",
    "    fig2.update_traces(marker=dict(size=7, opacity=0.8))\n",
    "    fig2.show()\n",
    "    fig2.write_html(os.path.join(OUT_DIR, \"barrier_clusters_by_label.html\"))\n"
   ],
   "id": "ef4edf77ff075713"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "barrier_counts = (\n",
    "    target_df[\"barrier_cluster_label\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace(\"\", \"unknown\")\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    ")\n",
    "barrier_counts.columns = [\"barrier_cluster_label\", \"n_articles\"]\n",
    "barrier_counts = barrier_counts[\n",
    "    ~barrier_counts[\"barrier_cluster_label\"].isin([\"other\", \"none\"])\n",
    "]\n",
    "fig = px.bar(\n",
    "    barrier_counts,\n",
    "    x=\"barrier_cluster_label\",\n",
    "    y=\"n_articles\",\n",
    "    title=\"Distribution of barrier clusters across all articles\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Barrier cluster label\",\n",
    "    yaxis_title=\"Number of articles\",\n",
    "    xaxis_tickangle=45,\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html(os.path.join(OUT_DIR, \"02_barrier_cluster_distribution_articles.html\"))\n",
    "\n"
   ],
   "id": "b7078398b7843b86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "target_df",
   "id": "910b8ebe32998b72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_plot = target_df.copy()\n",
    "df_plot[\"barrier_cluster_label\"] = (\n",
    "    df_plot[\"barrier_cluster_label\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace(\"\", \"unknown\")\n",
    ")\n",
    "df_plot = df_plot[\n",
    "    ~df_plot[\"barrier_cluster_label\"].isin([\"other\", \"none\"])\n",
    "]\n",
    "\n",
    "# Extract year from published_date (assuming it's a datetime or parseable string)\n",
    "df_plot[\"year\"] = pd.to_datetime(df_plot[\"published_date\"]).dt.year\n",
    "\n",
    "# Drop any rows with invalid years if needed (optional)\n",
    "df_plot = df_plot.dropna(subset=[\"year\"])\n",
    "df_plot[\"year\"] = df_plot[\"year\"].astype(int)\n",
    "\n",
    "# Count articles per year and per barrier cluster\n",
    "counts = (\n",
    "    df_plot.groupby([\"year\", \"barrier_cluster_label\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"n_articles\")\n",
    ")\n",
    "\n",
    "# Total articles per year\n",
    "year_totals = counts.groupby(\"year\")[\"n_articles\"].sum().reset_index(name=\"total_per_year\")\n",
    "\n",
    "# Merge and compute percentage\n",
    "counts = counts.merge(year_totals, on=\"year\")\n",
    "counts[\"percentage\"] = (counts[\"n_articles\"] / counts[\"total_per_year\"]) * 100\n",
    "\n",
    "# Create the plot: stacked bar chart is often best for showing ratios/proportions over time\n",
    "fig = px.bar(\n",
    "    counts,\n",
    "    x=\"year\",\n",
    "    y=\"percentage\",\n",
    "    color=\"barrier_cluster_label\",\n",
    "    title=\"Percentage of articles per barrier cluster by year\",\n",
    "    labels={\n",
    "        \"percentage\": \"Percentage of articles (%)\",\n",
    "        \"year\": \"Year\",\n",
    "        \"barrier_cluster_label\": \"Barrier cluster label\"\n",
    "    },\n",
    "    text=\"percentage\",  # Optional: show percentages on bars\n",
    "    barmode=\"stack\"  # Stacked to visualize proportions (sums to 100% per year)\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate=\"%{y:.1f}%\", textposition=\"inside\")\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"Percentage of articles (%)\",\n",
    "    xaxis_title=\"Year\",\n",
    "    legend_title=\"Barrier cluster\",\n",
    "    xaxis={\"type\": \"category\"},  # Treat year as categorical to avoid gaps\n",
    "    bargap=0.15\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "79f5f02def15bfd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Use the target DataFrame: target_df\n",
    "df_plot = df.copy()\n",
    "\n",
    "# Convert published_date to datetime and extract year\n",
    "df_plot[\"published_date\"] = pd.to_datetime(df_plot[\"published_date\"])\n",
    "df_plot[\"year\"] = df_plot[\"published_date\"].dt.year\n",
    "\n",
    "# Drop invalid years (if any)\n",
    "df_plot = df_plot.dropna(subset=[\"year\"])\n",
    "df_plot[\"year\"] = df_plot[\"year\"].astype(int)\n",
    "\n",
    "# Create a readable label for is_ai_llm (assuming it's boolean, 0/1, or similar)\n",
    "df_plot[\"category\"] = df_plot[\"is_ai_llm\"].apply(\n",
    "    lambda x: \"AI/LLM Related\" if x else \"Not AI/LLM\"\n",
    ")\n",
    "\n",
    "# Counts by year and category\n",
    "counts = (\n",
    "    df_plot.groupby([\"year\", \"category\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"n_articles\")\n",
    ")\n",
    "\n",
    "# Total articles per year\n",
    "year_totals = counts.groupby(\"year\")[\"n_articles\"].sum().reset_index(name=\"total_per_year\")\n",
    "\n",
    "# Merge and calculate percentage\n",
    "counts = counts.merge(year_totals, on=\"year\")\n",
    "counts[\"percentage\"] = (counts[\"n_articles\"] / counts[\"total_per_year\"]) * 100\n",
    "\n",
    "# Plot 1: Percentage of AI/LLM articles by year (stacked bar, sums to 100%)\n",
    "fig_pct = px.bar(\n",
    "    counts,\n",
    "    x=\"year\",\n",
    "    y=\"percentage\",\n",
    "    color=\"category\",\n",
    "    title=\"Percentage of AI/LLM-related articles by year (target_df)\",\n",
    "    labels={\n",
    "        \"percentage\": \"Percentage of articles (%)\",\n",
    "        \"year\": \"Year\",\n",
    "        \"category\": \"Article Type\"\n",
    "    },\n",
    "    barmode=\"stack\",\n",
    "    color_discrete_sequence=[\"#ff7f0e\", \"#1f77b4\"]  # Orange for AI, blue for non-AI\n",
    ")\n",
    "\n",
    "fig_pct.update_traces(texttemplate=\"%{y:.1f}%\", textposition=\"inside\")\n",
    "fig_pct.update_layout(\n",
    "    yaxis_title=\"Percentage (%)\",\n",
    "    xaxis_title=\"Year\",\n",
    "    legend_title=\"Article Type\",\n",
    "    xaxis={\"type\": \"category\"},\n",
    "    bargap=0.15\n",
    ")\n",
    "\n",
    "fig_pct.show()\n",
    "fig_pct.write_html(os.path.join(OUT_DIR, \"04_ai_llm_percentage_by_year.html\"))\n",
    "\n",
    "# Plot 2: Absolute number of articles by year (stacked bar)\n",
    "fig_count = px.bar(\n",
    "    counts,\n",
    "    x=\"year\",\n",
    "    y=\"n_articles\",\n",
    "    color=\"category\",\n",
    "    title=\"Number of AI/LLM-related articles by year (target_df)\",\n",
    "    labels={\n",
    "        \"n_articles\": \"Number of articles\",\n",
    "        \"year\": \"Year\",\n",
    "        \"category\": \"Article Type\"\n",
    "    },\n",
    "    barmode=\"stack\",\n",
    "    color_discrete_sequence=[\"#ff7f0e\", \"#1f77b4\"]\n",
    ")\n",
    "\n",
    "fig_count.update_layout(\n",
    "    yaxis_title=\"Number of articles\",\n",
    "    xaxis_title=\"Year\",\n",
    "    legend_title=\"Article Type\",\n",
    "    xaxis={\"type\": \"category\"}\n",
    ")\n",
    "\n",
    "fig_count.show()\n",
    "fig_count.write_html(os.path.join(OUT_DIR, \"05_ai_llm_counts_by_year.html\"))"
   ],
   "id": "4700d0a2d1d832b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# topic and barrier 2D compare plot\n",
    "df_plot = target_df.copy()\n",
    "\n",
    "# Clean topic and barrier labels\n",
    "df_plot[\"ai_topic_llm\"] = (\n",
    "    df_plot[\"ai_topic_llm\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace(\"\", \"unknown\")\n",
    ")\n",
    "\n",
    "df_plot[\"barrier_cluster_label\"] = (\n",
    "    df_plot[\"barrier_cluster_label\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .replace(\"\", \"unknown\")\n",
    ")\n",
    "\n",
    "# Remove aggregated / non-informative labels\n",
    "df_plot = df_plot[\n",
    "    ~df_plot[\"barrier_cluster_label\"].isin([\"other\", \"none\", \"unknown\"])\n",
    "]\n",
    "\n",
    "# Optional: also drop unknown topics\n",
    "df_plot = df_plot[df_plot[\"ai_topic_llm\"] != \"unknown\"]\n",
    "\n",
    "# Limit to top topics / top barriers to keep plot readable\n",
    "TOPK_TOPICS = 20\n",
    "TOPK_BARRIERS = 10\n",
    "\n",
    "top_topics = (\n",
    "    df_plot[\"ai_topic_llm\"]\n",
    "    .value_counts()\n",
    "    .head(TOPK_TOPICS)\n",
    "    .index\n",
    ")\n",
    "\n",
    "top_barriers = (\n",
    "    df_plot[\"barrier_cluster_label\"]\n",
    "    .value_counts()\n",
    "    .head(TOPK_BARRIERS)\n",
    "    .index\n",
    ")\n",
    "\n",
    "df_plot = df_plot[\n",
    "    df_plot[\"ai_topic_llm\"].isin(top_topics)\n",
    "    & df_plot[\"barrier_cluster_label\"].isin(top_barriers)\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 2. Topic × barrier table (counts + row %)\n",
    "# ============================================================\n",
    "\n",
    "# Cross-tab: counts\n",
    "ct = pd.crosstab(\n",
    "    df_plot[\"ai_topic_llm\"],\n",
    "    df_plot[\"barrier_cluster_label\"]\n",
    ")\n",
    "\n",
    "# Row-normalised percentages (share of barriers *within each topic*)\n",
    "ct_row_pct = ct.div(ct.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Long-format table (if you want to export / inspect)\n",
    "topic_barrier_long = (\n",
    "    ct\n",
    "    .stack()\n",
    "    .reset_index(name=\"n_articles\")\n",
    "    .rename(columns={\"ai_topic_llm\": \"topic\", \"barrier_cluster_label\": \"barrier\"})\n",
    ")\n",
    "\n",
    "topic_barrier_long[\"row_pct\"] = (\n",
    "    topic_barrier_long[\"n_articles\"]\n",
    "    / topic_barrier_long.groupby(\"topic\")[\"n_articles\"].transform(\"sum\")\n",
    "    * 100\n",
    ")\n",
    "\n",
    "print(\"\\n=== Topic × Barrier: counts (Top topics/barriers) ===\")\n",
    "print(ct)\n",
    "\n",
    "print(\"\\n=== Topic × Barrier: row-normalised % ===\")\n",
    "print(ct_row_pct.round(1))\n",
    "\n",
    "# Optional: save tables\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "ct.to_csv(\"results/topic_barrier_counts.csv\")\n",
    "ct_row_pct.to_csv(\"results/topic_barrier_row_pct.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. 2D heatmap: topic vs barrier (row %)\n",
    "# ============================================================\n",
    "\n",
    "fig = px.imshow(\n",
    "    ct_row_pct,\n",
    "    x=ct_row_pct.columns,\n",
    "    y=ct_row_pct.index,\n",
    "    text_auto=\".1f\",\n",
    "    aspect=\"auto\",\n",
    "    labels=dict(\n",
    "        x=\"Barrier cluster label\",\n",
    "        y=\"AI topic (ai_topic_llm)\",\n",
    "        color=\"Share within topic (%)\",\n",
    "    ),\n",
    "    title=\"Barrier distribution within AI topics (row-normalised %)\",\n",
    ")\n",
    "\n",
    "# If you prefer the x-axis at bottom, comment out the next line\n",
    "# fig.update_xaxes(side=\"top\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=45,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"results/topic_barrier_heatmap_row_pct.html\")\n",
    "\n",
    "# ============================================================\n",
    "# (Optional) 2D heatmap with raw counts instead of %\n",
    "# ============================================================\n",
    "\n",
    "fig_counts = px.imshow(\n",
    "    ct,\n",
    "    x=ct.columns,\n",
    "    y=ct.index,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    labels=dict(\n",
    "        x=\"Barrier cluster label\",\n",
    "        y=\"AI topic (ai_topic_llm)\",\n",
    "        color=\"Number of articles\",\n",
    "    ),\n",
    "    title=\"Barrier counts within AI topics (raw counts)\",\n",
    ")\n",
    "\n",
    "fig_counts.update_layout(\n",
    "    xaxis_tickangle=45,\n",
    ")\n",
    "\n",
    "fig_counts.show()"
   ],
   "id": "197d0b2f2a8e6453"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
