{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cuda\")\n",
    "queries = [\n",
    "    \"Artiklen undersøger kunstig intelligens og machine learning.\",\n",
    "    \"Teksten handler om AI-politik, regulering og etik.\",\n",
    "    \"Dette dokument beskriver AI-teknologi, GPT og automatisering.\",\n",
    "    \"Artiklen diskuterer brugen af robotter og generativ AI.\"\n",
    "]\n",
    "\n",
    "keywords_query = [\"AI\", \"kunstig intelligens\", \"maskinlæring\", \"dyb læring\", \"neural netværk\", \"automatisering\",\n",
    "                  \"robotik\", \"dataanalyse\", \"algoritme\", \"intelligente systemer\", \"GPT\", \"OPENAI\", \"LLM\", \"chatbot\",\n",
    "                    \"sprogmodel\", \"generativ AI\", \"AI-assistent\", \"AI-drevet\", \"computer vision\", \"naturlig sprogbehandling\",\n",
    "                    \"AI-platform\", \"AI-teknologi\", \"AI-forskning\", \"AI-innovation\", \"AI-applikationer\", \"AI-løsninger\",\n",
    "                    \"AI-udvikling\", \"AI-sikkerhed\", \"AI-etik\", \"AI-regulering\", \"AI-politik\", \"AI-strategi\", \"AI-investering\",\n",
    "                    \"AI-startup\", \"AI-industrien\", \"AI-marked\", \"AI-trends\", \"AI-fremtid\", \"robotter\", \"automatiserede systemer\",\n",
    "                    \"intelligente maskiner\", \"AI-integration\", \"AI-implementering\", \"AI-optimering\", \"AI-overvågning\",]"
   ],
   "id": "a26be8f30568151"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "emb = np.load(\"../results/01_embeddings.npy\")\n",
    "df = pd.read_csv(\"../results/00_raw_data.csv\")\n",
    "ai_vec = model.encode(queries, normalize_embeddings=True)\n",
    "query_vec = np.mean(ai_vec, axis=0)\n",
    "\n",
    "true_score =  util.cos_sim(emb, ai_vec)\n",
    "ai_final_score = true_score.mean(dim=1)\n",
    "# 1) 先把 keywords 分成 “短词” 和 “长词”\n",
    "short_keywords = [kw for kw in keywords_query\n",
    "                  if len(kw) <= 3 and \" \" not in kw]\n",
    "long_keywords = [kw for kw in keywords_query\n",
    "                 if kw not in short_keywords]\n",
    "\n",
    "short_patterns = {\n",
    "    kw: re.compile(r\"\\b\" + re.escape(kw.lower()) + r\"\\b\", flags=re.IGNORECASE)\n",
    "    for kw in short_keywords\n",
    "}\n",
    "\n",
    "keyword_scores = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text = row['plain_text']\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # 长关键词：子串匹配即可\n",
    "    score += sum(\n",
    "        1 for kw in long_keywords\n",
    "        if kw.lower() in text_lower\n",
    "    )\n",
    "\n",
    "    score += sum(\n",
    "        1 for kw, pattern in short_patterns.items()\n",
    "        if pattern.search(text_lower)\n",
    "    )\n",
    "\n",
    "    keyword_scores.append(score)\n",
    "\n",
    "keyword_scores = np.array(keyword_scores)\n",
    "# assign to column\n",
    "df['keyword_score'] = keyword_scores\n",
    "\n",
    "\n",
    "df[\"ai_score\"] = ai_final_score\n",
    "\n",
    "df[\"is_ai\"] = df[\"ai_score\"] > 0\n",
    "# and keyword score > 0\n",
    "df[\"is_ai\"] = df[\"is_ai\"] & (df[\"keyword_score\"] > 0)\n",
    "\n",
    "df.to_csv(\"results/02_ai_scores_v2.csv\", index=False)\n",
    "\n",
    "print(\"AI-related articles:\", df[\"is_ai\"].sum())\n",
    "\n"
   ],
   "id": "bb08c4f76f099c86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ai score vs keyword score correlation\n",
    "correlation = np.corrcoef(df[\"ai_score\"], df[\"keyword_score\"])[0, 1]\n",
    "print(\"Correlation between AI score and Keyword score:\", correlation)\n",
    "\n"
   ],
   "id": "91d4abb44a423372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot ai score vs keyword score distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df[\"ai_score\"], df[\"keyword_score\"], alpha=0.5)\n",
    "plt.title(\"AI Score vs Keyword Score Distribution\")\n",
    "plt.xlabel(\"AI Score\")\n",
    "plt.ylabel(\"Keyword Score\")\n",
    "plt.savefig(\"figures/ai_score_vs_keyword_score.png\")\n",
    "plt.show()"
   ],
   "id": "d30e685e23f1c302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ollama\n",
    "\n",
    "# 从最高 ai_score 的前 500 条里随机 100 条\n",
    "sample_ai = (\n",
    "    df.sort_values(by=\"ai_score\", ascending=False)\n",
    "      .iloc[:500]\n",
    "      .sample(n=100, random_state=42)\n",
    "      .copy()\n",
    ")\n",
    "sample_ai[\"score_source\"] = \"ai_score\"        # 标记来源\n",
    "sample_ai[\"score_bucket\"] = \"top500\"          # 想分桶就简单写个 tag\n",
    "\n",
    "\n",
    "\n",
    "# 合并\n",
    "sample_df = sample_ai\n",
    "\n",
    "print(f\"Total samples for Ollama: {len(sample_df)}\")  # 应该是 200\n",
    "sample_df.to_csv(\"results/03_ai_sample_for_ollama_top500.csv\", index=False)\n",
    "print(\"Saved sample to results/03_ai_sample_for_ollama_top500.csv\")\n",
    "\n"
   ],
   "id": "e7849aae49938dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df[df.keyword_score>0].shape",
   "id": "d0f1801ae5bc8510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# length of plain_text distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df[\"plain_text\"].dropna().apply(len), bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of Plain Text Lengths in Sampled Articles\")\n",
    "plt.xlabel(\"Length of Plain Text (characters)\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.show()"
   ],
   "id": "bc46bf533165a51a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. 先生成两个方法自己的二值预测（阈值先用 > 0，之后你可以调）\n",
    "results_df[\"pred_ai_score\"] = results_df[\"ai_score\"].fillna(0) > 0\n",
    "\n",
    "# 2. 按来源分别创建 mask\n",
    "mask_ai = results_df[\"score_source\"] == \"ai_score\"\n",
    "\n",
    "# 3. 对于 ai_score 抽出来的那 100 条，用 pred_ai_score 和 Ollama 比\n",
    "acc_ai = (results_df.loc[mask_ai, \"pred_ai_score\"] ==\n",
    "          results_df.loc[mask_ai, \"is_ai_ollama\"]).mean()\n",
    "\n",
    "\n",
    "print(f\"Accuracy for top500-high ai_score (threshold > 0): {acc_ai * 100:.2f}%\")\n",
    "\n"
   ],
   "id": "bf9e8564da9a63e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = []\n",
    "\n",
    "def check_ai_via_ollama(text: str) -> bool:\n",
    "    prompt = (\n",
    "        \"You are a strict AI detector.\\n\"\n",
    "        \"Question: Is the following article mainly about artificial intelligence (AI), \"\n",
    "        \"machine learning, GPT, or automation?\\n\"\n",
    "        \"Answer strictly with only one word: 'ja' for yes and 'nej' for no.\\n\\n\"\n",
    "        f\"Article:\\n{text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(\n",
    "\n",
    "        model=\"gemma3:latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = response[\"message\"][\"content\"].strip().lower()\n",
    "    if content.startswith(\"ja\"):\n",
    "        return True\n",
    "    if content.startswith(\"nej\"):\n",
    "        return False\n",
    "    # 不听话就当不是AI\n",
    "    return False\n",
    "\n",
    "\n",
    "# use tqdm for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "for index, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "    text = (row[\"plain_text\"] or \"\")[:2000]\n",
    "\n",
    "    is_ai = check_ai_via_ollama(text)\n",
    "\n",
    "    results.append({\n",
    "        \"orig_index\": row.name,                  # 原始df的index\n",
    "        \"ai_score\": row[\"ai_score\"],\n",
    "        \"keyword_score\": row[\"keyword_score\"],\n",
    "        \"is_ai_ollama\": is_ai,\n",
    "        \"total_length\": len(text),\n",
    "        \"score_source\": row[\"score_source\"],\n",
    "        \"score_bucket\": row[\"score_bucket\"],\n",
    "  # 属于哪个百分位区间\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ],
   "id": "e5a0eff27f4456bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results_df",
   "id": "d46bf8f16e62d343"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find those ai ollama true in original df\n",
    "ai_indices = results_df[results_df[\"is_ai_ollama\"] == True][\"orig_index\"].unique()\n",
    "# output that plain text to ternimal\n",
    "for idx in ai_indices:\n",
    "\n",
    "    print(sample_df.loc[idx, \"plain_text\"])\n",
    "    print(\"-\" * 80)\n"
   ],
   "id": "89b16e63a62f53f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def auto_thresholds_from_scores(scores, n=7, trim_ends=True):\n",
    "\n",
    "    scores = np.asarray(scores)\n",
    "    scores = scores[~np.isnan(scores)]\n",
    "\n",
    "    if scores.size == 0:\n",
    "        return []\n",
    "\n",
    "    if trim_ends:\n",
    "        # 例如 n=7 → 取 10%, 20%, ..., 80%, 90%\n",
    "        qs = np.linspace(0.1, 0.9, n)\n",
    "    else:\n",
    "        qs = np.linspace(0.0, 1.0, n+2)[1:-1]  # 去掉 0 和 1\n",
    "\n",
    "    thr = np.quantile(scores, qs)\n",
    "    thr = np.unique(thr)  # 去重一下\n",
    "    return thr\n"
   ],
   "id": "6b31a4560032f698"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Full evaluation script:\n",
    "- Sample data in several different ways\n",
    "- Call Ollama to label whether each article is AI-related\n",
    "- Compare ai_score vs ai_score_simple\n",
    "- Collect all metrics into a single report table (DataFrame + CSV)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. CONFIG\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "MODEL_NAME = \"gemma3:latest\"    # Ollama model name\n",
    "MAX_TEXT_LEN = 2000             # 每篇文章截断长度（字符数）\n",
    "GLOBAL_RANDOM_N = 300           # 全局随机抽样数量\n",
    "TOP_K = 500                     # topK 高分池\n",
    "TOP_SAMPLE_N = 100              # 从 topK 中随机抽多少\n",
    "STRAT_N_BINS = 5                # 分位桶数（0-20,20-40,...）\n",
    "STRAT_N_PER_BIN = 60            # 每个分位桶抽多少\n",
    "DISAGREE_N_PER_ZONE = 80        # 每种“分歧类型”抽多少\n",
    "DISAGREE_HIGH_T = 0.08          # “高分”阈值（ai 高 / simple 高）\n",
    "DISAGREE_LOW_T = 0.0            # “低分”阈值（ai 低 / simple 低）\n",
    "THRESHOLDS = auto_thresholds_from_scores(df['ai_score'], n=5, trim_ends=True) # 用来扫不同阈值的列表\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. 报表收集器：所有指标都往这里塞\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "report_rows = []\n",
    "\n",
    "def add_row(subset, method, bucket, metric, value, n_samples):\n",
    "    \"\"\"\n",
    "    subset: 哪类样本（global_random / top500_high / stratified_quintile / disagree 等）\n",
    "    method: 哪个打分方法（ai_score / ai_score_simple）\n",
    "    bucket: 子分组，比如 'all' 或 '0-20%' / '80-100%' / 'ai_high_simple_low'\n",
    "    metric: 指标名称，比如 'auc', 'accuracy_thr>0.05'\n",
    "    value: 指标值（float）\n",
    "    n_samples: 该组样本数\n",
    "    \"\"\"\n",
    "    report_rows.append({\n",
    "        \"subset\": subset,\n",
    "        \"method\": method,\n",
    "        \"bucket\": bucket,\n",
    "        \"metric\": metric,\n",
    "        \"value\": float(value),\n",
    "        \"n_samples\": int(n_samples),\n",
    "    })\n",
    "\n",
    "def build_report_df():\n",
    "    return pd.DataFrame(report_rows)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Ollama 调用：给一段文本返回 True/False（是否 AI 相关）\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def check_ai_via_ollama(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    用 Ollama 问一句：这篇文章是不是关于 AI/ML/GPT/自动化？\n",
    "    返回 True(ja) / False(nej)\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a strict AI detector.\\n\"\n",
    "        \"Question: Is the following article mainly about artificial intelligence (AI), \"\n",
    "        \"machine learning, GPT, or automation?\\n\"\n",
    "        \"Answer strictly with only one word: 'ja' for yes and 'nej' for no.\\n\\n\"\n",
    "        f\"Article:\\n{text}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = response[\"message\"][\"content\"].strip().lower()\n",
    "    if content.startswith(\"ja\"):\n",
    "        return True\n",
    "    if content.startswith(\"nej\"):\n",
    "        return False\n",
    "    # 如果模型不听话，保守当作 False\n",
    "    return False\n",
    "\n",
    "def label_with_ollama(sample_df: pd.DataFrame, subset_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    对 sample_df 的每一行用 Ollama 打标签。\n",
    "    保留打分、来源、桶信息，并新增 is_ai_ollama 和 subset。\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, row in tqdm(sample_df.iterrows(), total=sample_df.shape[0], desc=f\"Ollama labeling [{subset_name}]\"):\n",
    "        text = (row.get(\"plain_text\", \"\") or \"\")[:MAX_TEXT_LEN]\n",
    "        is_ai = check_ai_via_ollama(text)\n",
    "\n",
    "        rows.append({\n",
    "            'orig_index': _,\n",
    "            \"subset\": subset_name,\n",
    "            \"ai_score\": row[\"ai_score\"],\n",
    "            \"keyword_score\": row.get(\"keyword_score\", np.nan),\n",
    "            \"is_ai_ollama\": is_ai,\n",
    "            \"score_source\": row.get(\"score_source\", \"both\"),\n",
    "            \"score_bucket\": row.get(\"score_bucket\", \"all\"),\n",
    "            \"zone\": row.get(\"zone\", \"all\"),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. 各种抽样方法\n",
    "#    注意：这里假设 df 已经在全局变量里存在\n",
    "#    df 至少需要列：plain_text, ai_score, ai_score_simple\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def sample_global_random(df: pd.DataFrame, n: int, random_state: int = 123) -> pd.DataFrame:\n",
    "    \"\"\"全局随机抽样 n 条\"\"\"\n",
    "    return df.sample(n=n, random_state=random_state).copy()\n",
    "\n",
    "def sample_top_k(df: pd.DataFrame, score_col: str, top_k: int, sample_n: int, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    从 score_col 得分最高的 top_k 条里，随机抽 sample_n 条。\n",
    "    \"\"\"\n",
    "    top = df.sort_values(score_col, ascending=False).iloc[:top_k].copy()\n",
    "    sample_n = min(sample_n, len(top))\n",
    "    sampled = top.sample(n=sample_n, random_state=random_state).copy()\n",
    "    sampled[\"score_source\"] = score_col\n",
    "    sampled[\"score_bucket\"] = f\"top{top_k}\"\n",
    "    return sampled\n",
    "\n",
    "def stratified_by_score_ranges(df: pd.DataFrame, score_col: str,\n",
    "                               n_bins: int = 5, n_per_bin: int = 60,\n",
    "                               random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    按 score_col 把数据切成 n_bins 个分位区间（0-20,20-40,...），\n",
    "    每个分位区间随机抽 n_per_bin 条。\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"orig_index\"] = df.index\n",
    "\n",
    "    labels = [f\"{int(100/n_bins*i)}-{int(100/n_bins*(i+1))}%\" for i in range(n_bins)]\n",
    "    bucket_col = \"score_bucket\"\n",
    "\n",
    "    df[bucket_col] = pd.qcut(\n",
    "        df[score_col],\n",
    "        q=n_bins,\n",
    "        labels=labels,\n",
    "        duplicates=\"drop\"\n",
    "    )\n",
    "\n",
    "    df = df.dropna(subset=[bucket_col])\n",
    "    samples = []\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    for b in df[bucket_col].dropna().unique():\n",
    "        sub = df[df[bucket_col] == b]\n",
    "        size = min(n_per_bin, len(sub))\n",
    "        if size <= 0:\n",
    "            continue\n",
    "        seed = int(rng.integers(0, 1e9))\n",
    "        s = sub.sample(n=size, random_state=seed).copy()\n",
    "        s[\"score_source\"] = score_col\n",
    "        samples.append(s)\n",
    "\n",
    "    return samples\n"
   ],
   "id": "bd7ec9cac5b925bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. 主流程：抽样 + Ollama 打标签\n",
    "#    注意：这里假设 df 已经定义好了\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# !!! 这里假设你之前已经加载好了 df !!!\n",
    "# df = pd.read_csv(\"your_news_data.csv\")  # 举例\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# 4.1 全局随机样本\n",
    "global_sample = sample_global_random(df[df['is_ai']], n=GLOBAL_RANDOM_N, random_state=123)\n",
    "global_results_df = label_with_ollama(global_sample, subset_name=\"global_random\")\n",
    "all_results.append(global_results_df)\n",
    "\n",
    "# 4.2 top500 高分样本（各方法自己 confident 的区域）\n",
    "sample_ai_top = sample_top_k(df[df['is_ai']], \"ai_score\", TOP_K, TOP_SAMPLE_N, random_state=42)\n",
    "top500_results_df = label_with_ollama(sample_ai_top, subset_name=\"top500_high\")\n",
    "all_results.append(top500_results_df)\n",
    "\n",
    "# 4.3 分位区间抽样（覆盖高/中/低不同区间）\n",
    "sample_ai_strat = stratified_by_score_ranges(df[df['is_ai']], \"ai_score\",\n",
    "                                             n_bins=STRAT_N_BINS,\n",
    "                                             n_per_bin=STRAT_N_PER_BIN,\n",
    "                                             random_state=1)\n",
    "strat_sample = pd.concat(sample_ai_strat)\n",
    "strat_results_df = label_with_ollama(strat_sample, subset_name=\"stratified_quintile\")\n",
    "all_results.append(strat_results_df)\n"
   ],
   "id": "5e559d70c64892b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "global_results_df.shape",
   "id": "44fd124bd94b24f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "561eba1ddd9e7b31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['is_ai'].sum()",
   "id": "9653ddf63fbe4a14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# min max normalize ai_score\\ai score simple  in all_results_df\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "all_results_df[\"ai_score_norm\"] = scaler1.fit_transform(all_results_df[[\"ai_score\"]])"
   ],
   "id": "de215c957ea669ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pp = all_results_df[all_results_df[\"is_ai_ollama\"] == True]\n",
    "print(all_results_df[['ai_score_norm']].mean())\n"
   ],
   "id": "e527dc0915fd278f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find index 305 text\n",
    "s = df.loc[1073867, \"plain_text\"]\n",
    "print(s)\n",
    "for x in keywords_query:\n",
    "    if x.lower() in s.lower():\n",
    "        print(f\"Found keyword: {x}\")"
   ],
   "id": "cdaa09fe6ecef9c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 合并所有结果\n",
    "all_results_df = pd.concat(all_results)\n",
    "\n",
    "all_results_df[\"score_bucket\"] = all_results_df[\"score_bucket\"].fillna(\"all\")\n",
    "all_results_df[\"zone\"] = all_results_df[\"zone\"].fillna(\"all\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. 各种评估：写入统一报表\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 5.1 全局随机样本：AUC + 多个阈值 accuracy\n",
    "global_res = all_results_df[all_results_df[\"subset\"] == \"global_random\"].copy()\n",
    "y_g = global_res[\"is_ai_ollama\"].astype(int)\n",
    "\n",
    "for method, col in [(\"ai_score\", \"ai_score\")]:\n",
    "    # AUC\n",
    "    auc_val = roc_auc_score(y_g, global_res[col])\n",
    "    add_row(\"global_random\", method, \"all\", \"auc\", auc_val, len(y_g))\n",
    "    # 不同阈值 accuracy\n",
    "    for thr in THRESHOLDS:\n",
    "        pred = global_res[col] > thr\n",
    "        acc_val = accuracy_score(y_g, pred)\n",
    "        add_row(\"global_random\", method, \"all\", f\"accuracy_thr>{thr:.2f}\", acc_val, len(y_g))\n",
    "\n",
    "# 5.2 top500 高分样本：看各自高分区域的 accuracy（阈值先用 >0）\n",
    "top_res = all_results_df[all_results_df[\"subset\"] == \"top500_high\"].copy()\n",
    "\n",
    "mask_ai_top = top_res[\"score_source\"] == \"ai_score\"\n",
    "\n",
    "y_ai_top = top_res.loc[mask_ai_top, \"is_ai_ollama\"].astype(int)\n",
    "\n",
    "pred_ai_top = top_res.loc[mask_ai_top, \"ai_score\"] > 0\n",
    "\n",
    "acc_ai_top = accuracy_score(y_ai_top, pred_ai_top)\n",
    "\n",
    "add_row(\"top500_high\", \"ai_score\", \"top500\", \"accuracy_thr>0.00\", acc_ai_top, len(y_ai_top))\n",
    "\n",
    "# 5.3 分位区间抽样：看不同 score_bucket 的 accuracy（阈值 >0）\n",
    "strat_res = all_results_df[all_results_df[\"subset\"] == \"stratified_quintile\"].copy()\n",
    "\n",
    "# 先构造两种方法的预测列\n",
    "strat_res[\"pred_ai_score\"] = strat_res[\"ai_score\"] > 0\n",
    "\n",
    "mask_ai_strat = strat_res[\"score_source\"] == \"ai_score\"\n",
    "\n",
    "# 对 ai_score 抽出来的样本\n",
    "sub_ai = strat_res[mask_ai_strat].copy()\n",
    "sub_ai[\"correct\"] = sub_ai[\"pred_ai_score\"] == sub_ai[\"is_ai_ollama\"]\n",
    "\n",
    "# 对 ai_score_simple 抽出来的样本\n",
    "\n",
    "# groupby 各自的 score_bucket\n",
    "g_ai = (\n",
    "    sub_ai.groupby(\"score_bucket\")[\"correct\"]\n",
    "    .agg([\"mean\", \"size\"])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"mean\": \"accuracy\", \"size\": \"n_samples\"})\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for _, r in g_ai.iterrows():\n",
    "    add_row(\"stratified_quintile\", \"ai_score\", r[\"score_bucket\"], \"accuracy_thr>0.00\", r[\"accuracy\"], r[\"n_samples\"])\n",
    "\n",
    "\n",
    "\n",
    "# 5.4 分歧样本：谁在“打架区域”更靠谱\n",
    "if \"disagree\" in all_results_df[\"subset\"].unique():\n",
    "    dis_res = all_results_df[all_results_df[\"subset\"] == \"disagree\"].copy()\n",
    "    y_d = dis_res[\"is_ai_ollama\"].astype(int)\n",
    "\n",
    "    # 两种方法用相同阈值 DISAGREE_HIGH_T\n",
    "    pred_ai_d = dis_res[\"ai_score\"] > DISAGREE_HIGH_T\n",
    "\n",
    "    acc_ai_d = accuracy_score(y_d, pred_ai_d)\n",
    "\n",
    "    add_row(\"disagree_all\", \"ai_score\", \"all\", f\"accuracy_thr>{DISAGREE_HIGH_T:.2f}\", acc_ai_d, len(y_d))\n",
    "\n",
    "    # 再按 zone 分开看（ai_high_simple_low / ai_low_simple_high）\n",
    "    for z, sub in dis_res.groupby(\"zone\"):\n",
    "        y_z = sub[\"is_ai_ollama\"].astype(int)\n",
    "        pred_ai_z = sub[\"ai_score\"] > DISAGREE_HIGH_T\n",
    "\n",
    "        acc_ai_z = accuracy_score(y_z, pred_ai_z)\n",
    "\n",
    "        add_row(f\"disagree_{z}\", \"ai_score\", \"all\", f\"accuracy_thr>{DISAGREE_HIGH_T:.2f}\", acc_ai_z, len(y_z))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. 生成总报表 & 导出\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "report_df = build_report_df()\n",
    "print(report_df)\n",
    "\n",
    "report_df.to_csv(\"results/ai_score_comparison_report.csv\", index=False)\n",
    "print(\"Saved report to results/ai_score_comparison_report.csv\")\n",
    "\n",
    "# 可选：把 all_results_df 也存一下，方便以后复查每条样本\n",
    "all_results_df.to_csv(\"results/ai_score_all_labeled_samples.csv\", index=False)\n",
    "print(\"Saved all labeled samples to results/ai_score_all_labeled_samples.csv\")\n"
   ],
   "id": "61303cb38e01458a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# find total number of ai_score > 0.35\n",
    "df.shape"
   ],
   "id": "a95b4ed25a0f4a13"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
