#Important : This is not part of final analysis. 
# -*- coding: utf-8 -*-
"""Semester project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fs_W-q98R5ut-J0btypItXIPrlyQYcsM
"""

!pip install spacy pandas matplotlib seaborn bertopic[flair] sentence-transformers
!python -m spacy download da_core_news_lg

# Step 0.1: Imports and constants
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import random
from collections import Counter

# Embedding & clustering
from umap import UMAP
from sklearn.cluster import KMeans
import hdbscan
from sentence_transformers import SentenceTransformer


# NLP
import spacy

# Reproducibility
RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

sns.set(style="whitegrid")

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Load embeddings from disk without loading all into RAM
embeddings = np.load('/content/drive/MyDrive/01_embeddings.npy', mmap_mode='r')
print("Embeddings shape (memory-mapped):", embeddings.shape)

df = pd.read_csv('/content/drive/MyDrive/danishnews.csv')

df.head()

df.isna().mean().sort_values(ascending=False)

df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')
df = df.dropna(subset=['published_date'])

df['year'] = df['published_date'].dt.year
df['year'].value_counts().sort_index().plot(kind='bar', figsize=(10,4))
plt.title("Number of articles per year")
plt.show()

AI_KEYWORDS = [
    "ai", "kunstlig intelligens", "artificial intelligence",
    "maskinlæring", "machine learning", "deep learning",
    "neurale netværk", "neural netværk",
    "algoritme", "algoritmer", "generativ ai",
    "chatbot", "chatbotter",
    "robot", "robotter", "robotik", "automatisering",
    "sprogmodel", "store sprogmodeller",
    "computer vision", "billedgenkendelse",
    "ansigtsgenkendelse",
]

import re

def build_regex(keyword_list):
    escaped = [re.escape(k.lower()) for k in keyword_list]
    return r"(" + r"|".join(escaped) + r")"

AI_PATTERN = build_regex(AI_KEYWORDS)

df['text_lower'] = df['plain_text'].str.lower()
df['is_ai'] = df['text_lower'].str.contains(AI_PATTERN, regex=True)

ai_counts = df[df['is_ai']].groupby('year').size()
print(ai_counts)

import matplotlib.pyplot as plt

plt.figure(figsize=(10,4))
ai_counts.plot(kind='bar', title="AI-Related News Articles Per Year (2016–2024)")
plt.xlabel("Year")
plt.ylabel("Number of AI Articles")
plt.xticks(rotation=45)
plt.show()

publisher_year_counts = df.groupby(['year', 'publisher']).size().reset_index(name='article_count')
publisher_year_counts.head()

top_publishers = publisher_year_counts.groupby('publisher')['article_count'].sum().nlargest(5).index

subset = publisher_year_counts[publisher_year_counts['publisher'].isin(top_publishers)]

plt.figure(figsize=(12,6))
for pub in top_publishers:
    yearly = subset[subset['publisher'] == pub]
    plt.plot(yearly['year'], yearly['article_count'], marker='o', label=pub)

plt.title("Top 5 News Publishers: Article Count Per Year")
plt.xlabel("Year")
plt.ylabel("Number of Articles")
plt.legend()
plt.grid(True)
plt.show()

BARRIERS_KEYWORDS = [
    "manglende data", "datasikkerhed", "privacy", "persondata",
    "lovgivning", "regulering", "compliance", "ai act",
    "ressourcemangel", "kompetencemangel", "dygtige medarbejdere",
    "omkostninger", "høje omkostninger", "teknologisk kompleksitet",
    "mangel på ekspertise", "modstand mod forandring", "etisk bekymring",
    "ansvarlig ai", "algoritmisk bias", "fejl i system", "misinformation",
    "overvågning", "jobtab", "usikkerhed", "risiko", "usikkerhed omkring ai", "resistance", "skills gaps", "data quality", "ethics"
]

import re

def build_regex(keyword_list):
    escaped = [re.escape(k.lower()) for k in keyword_list]
    return r"(" + r"|".join(escaped) + r")"

BARRIERS_PATTERN = build_regex(BARRIERS_KEYWORDS)

# Lowercase text for matching
df['text_lower'] = df['plain_text'].str.lower()

# Boolean flag: mentions any barrier
df['mentions_barrier'] = df['text_lower'].str.contains(BARRIERS_PATTERN, regex=True)

barrier_counts = df[df['mentions_barrier']].groupby('year').size()
print(barrier_counts)

from collections import Counter

barrier_texts = df[df['mentions_barrier']]['text_lower']
all_words = " ".join(barrier_texts).split()
word_freq = Counter(all_words)

# Show most common barrier-related words
for word, count in word_freq.most_common(30):
    if word in [w.lower() for w in BARRIERS_KEYWORDS]:  # optional filter
        print(word, count)

import numpy as np

embeddings = np.load('/content/drive/MyDrive/01_embeddings.npy', mmap_mode='r')

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

event_queries = {
    "research_innovation": [
        "AI research", "machine learning research",
        "university AI lab", "deep learning breakthrough",
        "scientific innovation in artificial intelligence"
    ],
    "industry_corporate_adoption": [
        "AI adoption in companies", "enterprise AI",
        "business automation with AI", "corporate AI strategy",
        "AI deployment in industry"
    ],
    "government_policy": [
        "AI public policy", "government AI strategy",
        "national AI regulations", "public sector AI adoption"
    ],
    "ethics_regulation": [
        "AI ethics", "AI regulation",
        "AI safety", "ethical implications of machine learning"
    ],
    "automation_workforce": [
        "job automation", "AI replacing jobs",
        "workforce transformation due to AI"
    ],
    "tech_development": [
        "machine learning model development",
        "AI system design", "neural networks",
        "AI product development"
    ],
    "startups_investment": [
        "AI startup", "venture capital AI investment",
        "AI funding", "AI business ecosystem"
    ],
    "education_skills": [
        "AI skills training", "digital skills",
        "AI education", "upskilling with artificial intelligence"
    ],
    "international_trends": [
        "global AI trends", "international AI development",
        "AI geopolitics", "AI race", "European AI initiatives"
    ]
}

def embed_queries(query_dict):
    event_vecs = {}
    for evt, phrases in query_dict.items():
        event_vecs[evt] = model.encode(phrases, convert_to_numpy=True, show_progress_bar=False)
    return event_vecs

event_query_vectors = embed_queries(event_queries)

from sklearn.metrics.pairwise import cosine_similarity

def compute_event_scores(emb_matrix, event_query_vectors, batch_size=5000):
    n = emb_matrix.shape[0]
    results = {evt: [] for evt in event_query_vectors}

    for i in range(0, n, batch_size):
        batch = emb_matrix[i:i+batch_size]

        for evt, qvecs in event_query_vectors.items():
            sims = cosine_similarity(batch, qvecs)
            mean_scores = sims.mean(axis=1)
            results[evt].extend(mean_scores)

        print(f"Processed {i+batch_size:,}/{n:,} rows")

    return results

event_scores = compute_event_scores(
    emb_matrix=embeddings,
    event_query_vectors=event_query_vectors,
    batch_size=5000
)

for evt, scores in event_scores.items():
    df[f"score_{evt}"] = scores

event = "industry_corporate_adoption"

df[["title", "plain_text", f"score_{event}"]].sort_values(
    by=f"score_{event}", ascending=False
).head(10)

df["event_type"] = df[
    [c for c in df.columns if c.startswith("score_")]
].idxmax(axis=1).str.replace("score_", "")

dominant_counts = df.groupby(["year", "event_type"]).size().unstack(fill_value=0)

dominant_counts.plot(figsize=(14, 7))
plt.title("Dominant AI Event Types in Danish News (Per Year)")
plt.ylabel("Number of articles")
plt.xlabel("Year")
plt.show()

score_cols = [c for c in df.columns if c.startswith("score_")]

if len(score_cols) == 0:
    print("No columns starting with 'score_' found in df.")
    print("Available columns:", list(df.columns[:50]))
    raise RuntimeError("Compute semantic scores first (see previous pipeline).")
else:
    print(f"Found {len(score_cols)} score columns.")
    print(score_cols)

# Cell 4 — distribution plot of scores (histograms)
plt.figure(figsize=(12,6))
for col in score_cols:
    plt.hist(df[col].dropna(), alpha=0.9, bins=60, label=col)
plt.title("Distribution of Semantic Relevance Scores by Event Type")
plt.xlabel("Semantic relevance score")
plt.ylabel("Article count")
plt.legend(loc='upper right')
plt.show()

plt.figure(figsize=(12, 6))

for col in score_cols:
    sns.kdeplot(df[col].dropna(), label=col, linewidth=2)

plt.title("Semantic Score Density per Event Type")
plt.xlabel("Score (0–1)")
plt.ylabel("Density")
plt.legend(loc="upper right")
plt.show()
